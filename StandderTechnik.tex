\section{Umfelderfassung}
\label{sec:TechnikStand}
Dieses Kapitel stellt zunächst die bestehenden Sensoren zur Umfelderfassung vor und erläutert deren Funktionsprinzipien. Anschließend wird auf die Sensordatenverarbeitung eingangen. Dazu gehören die Datenfusion, die Objekterkennung und das Tracking. Auf diesen Grundlagen aufbauend wird in den Kapiteln \ref{sec:KFZSensor} und \ref{sec:InfraSensor} aufgezeigt, wie dies im Fahrzeug und auch in der Infrastruktur genutzt wird.

\subsection{Sensoren zur Umfelderfassung}
\label{sec:Sensoren}
Die Sensoren zur Umfelderfassung werden in entfernungsgebende und bildgebende Sensoren unterschieden. Zu ersterem gehören der Ultraschall, der Radar und der Lidar. Zu letzterem die Kamera mit dem sichtbaren und dem Infrarotspektrum. Im Folgendem werden die einzelnen Sensorprinzipien erläutert.

\subsubsection{Ultraschall}
\label{sec:Ultraschall}

Als Ultraschall werden die Schallfrequenzen ab \unit{20}{kHz} bezeichnet. Sie gehören zu den Frequenzen, die für das menschliche Ohr nicht hörbar sind. Die Messung mit Ultraschall gehört zu den Laufzeitmessungen. Ein Sender emittiert Schallwellen, die schließlich von Objekten reflektiert werden. Mit Hilfe der gemessenen Laufzeit $\Delta t$ bis das Echo wieder am Sender ankommt kann der Abstand $d$ zum gemessenen Objekt bestimmt werden \cite{Trankler.2014}:

\begin{equation}
d = \dfrac{c_S \Delta t}{2}.
\label{eq:AbstandU}
\end{equation}

Mit der Schallgeschwindigkeit $c_S$. Da die Strecke zwischen Sender und Objekt zweimal durchlaufen wird, muss diese halbiert werden um den tatsächlichen Abstand zu erhalten. Neben der reinen Abstandsmessung kann mit Hilfe von zwei Sendern, deren Erfassungsbereiche überlappen, auch eine Positionsbestimmung durchgeführt werden. Hierzu wird das Trilaterationsverfahren genutzt. Für ein rundes Objekt ist dies in Abbildung \ref{fig:Trilateration} dargestellt.\marginpar{Abbildung überarbeiten!} Der Abstand $D$ berechnet sich mittels des Satzes von Pythagoras:

\begin{equation}
D = \sqrt{DE1^2 - \dfrac{\left(d^2 + DE1^2-DE2^2\right)^2}{4d^2}}
\label{eq:PosD}
\end{equation}

\begin{figure}%
\centering
\includegraphics[width=0.6\textwidth,trim={2cm 11cm 16cm 3.5cm},clip]{pics/Position_Ultraschall.pdf}%
\caption{Veranschaulichung des Trilaterationsprinzips für ein rundes Objekt}%
\label{fig:Trilateration}%
\end{figure}

Die Reichweite des Ultraschallsensors ist abhängig von der ausgesendeten Schallintensität $I_S$, da die Schallintensität in Abhängigkeit von der Entfernung $r$ des gemessenen Objektes abnimmt. Somit ergibt sich, mit der effektiven Reflexionsfläche $\sigma$ und bezogen auf den Normabstand $r_1$, die reflektierte Schallintensität 

\begin{equation}
I_{refl}=\sigma I_s \left(\dfrac{r_1}{2r}\right)^2.
\label{eq:Irefl}
\end{equation}

Des Weiteren verringert der Reflexionsgrad $\rho_S$ und die Impedanz der Atmosphäre die Schallintensität bei der Reflexion. Als untere Grenze zur Objektmessung muss die Schallintensität des Empfangssignals oberhalb des Rauschens liegen, d.h. \unit{$\geq$10}{dB} sein.

Zur Schallerzeugung und -empfang wird bei Ultraschallsensoren eine Membran aus einer Piezokeramik eingesetzt \cite{Winner.2015}. Zum Aussenden der Schallwellen wird die Membran aktiv in Schwingung versetzt und nach einer festgelegten Sendedauer wieder zur Ruhe gebracht. Die Zeit bis der reflektierte Schall die Membran wieder zur Schwingung anregt wird mit Gleichung \ref{eq:AbstandU} zur Abstandsbestimmung genutzt.

\subsubsection{Radar}
\label{sec:Radar}

Die Radarmessung (Radio Detection And Ranging) gehört zu den berührungslosen Messverfahren und wird insbesondere bei anspruchsvollen Umgebungsbedingungen eingesetzt \cite{Trankler.2014}. Bei diesem Verfahren werden elektromagnetische Wellen im Mikrowellenbereich eingesetzt, welche kaum anfällig gegenüber Temperaturschwankungen und Nebel sind. Für den Automobilbereich sind die \unit{24}{GHz} und \unit{77}{GHz} Frequenzen reserviert \cite{Winner.2015}.

Zur Abstandsmessung finden zwei verschiedene Ansätze Verwendung, die sich in der Frequenzmodulation unterscheiden. Das ist einmal das Dauerstrichradar (FMCW) und zum anderen die Chirp Frequence Modulation. Die Frequenzverläufe der beiden Modulationsverfahren sind in Abbildung \marginpar{Abbildung einfügen!} dargestellt. Das Dauerstrichradar erzeugt durch die kontinuierliche und rampenförmige Veränderung der Momentanfrequenz eine konstante Phasenverschiebung von $+(2r/c)^2m_{\omega}$. Der Abstand und die Geschwindigkeit wird bei diesem Verfahren mithilfe der Frequenzverschiebung bestimmt:

\begin{align}
	r &= \dfrac{c}{2} \cdot \dfrac{\omega_{obj,1}-\omega_{obj,2}}{m_{\omega,1} - m_{\omega,2}},\\
	\dot{r} &= \dfrac{c}{2 \omega_0} \cdot \dfrac{m_{\omega,1}\omega_{obj,1}-m_{\omega,2}\omega_{obj,2}}{m_{\omega,1} - m_{\omega,2}}.
	\end{align}

Bei der Chirp Frequence Modulation wird ein Sägezahnsignal mit einem Hub von $f_{chirp}=30...300\,\text{MHz}$ erzeugt. Die Dopplerfrequenz bestimmt hierbei die Wiederholrate und sollte etwa \unit{80}{kHz} betragen, um Mehrdeutigkeiten zu vermeiden. Der Abstand wird mit dem Puls-Doppler-Verfahren mit der Laufzeit $t_{of}=t_{PC}-t_S$, bezogen auf die Pulsmitte $t_{PC}$, bestimmt:

\begin{equation}
	r = \dfrac{1}{2}c t_{of}
\label{eq:AbstandRadar}
\end{equation}

Für die Bestimmung der Geschwindigkeit wird beim Radar der Doppler-Effekt genutzt. Der Doppler-Effekt besagt, dass sich die Frequenz bei der Reflexion in Abhängigkeit von der Änderung des Abstandes $\dot{r}$ ändert. Diese Frequenz wird Dopplerfrequenz $f_{Doppler}$ genannt und ergibt sich mit der Trägerfrequenz $f_0$ und der Lichtgeschwindigkeit $c$ folgendermaßen:

\begin{equation}
f_{Doppler} = -\dfrac{2 \dot{r}}{c} f_0
\label{eq:Doppler}
\end{equation}

Bei einer Annäherung ($\dot{r}<0$) ist diese Frequenz positiv und beim Entfernen negativ.

Die Reichweite des Radars ist abhängig von der Sendeleistung und der Richtcharakteristik der Antenne. Je nach Richtcharakteristik ergibt sich der Antennengewinn $G$, der Einfluss auf die Reichweite nimmt. So ergibt sich die Empfangsleistung für ein reflektiertes Radarsignal zu

\begin{equation}
P_R = \dfrac{10^{-2kr/1000}  \sigma  \lambda^2  G^2  V_{mp}^2  P_{total}}{(4\pi)^3 r^4}
\label{eq:Empfangsleistung}
\end{equation}

mit dem Rückstreuquerschnitt des Objektes

\begin{equation}
\sigma_{plate} = 4\pi \dfrac{A^2}{\lambda^2}.
\label{eq:Rueckstreuquerschnitt}
\end{equation}

Gleichung \ref{eq:Empfangsleistung} berücksichtigt außerdem sogenannte Signalleistungsschüttler mit dem Faktor $V_{mp}^2$, $0 \leq V_{mp} \leq 2$.

Für die Winkelbestimmung kommen zwei verschiedene Verfahren zum Einsatz. Das ist zum Einen das mechanisch Scanning, bei dem eine Strahlablenkeinheit oder eine Planarantenne mechanisch geschwenkt wird. Dabei bewegt sich die Radarkeule mit einer Schrittweite von etwa \unit{1}{°}, siehe Abbildung \marginpar{Abbildung einfügen}. Das zweite Verfahren ist das Monopuls-Verfahren. Hierbei erzeugt eine separate Antenne einen Sendestrahl, der von einer Doppelantennen-Anordnung Empfangen wird, siehe Abbildung \marginpar{Abbildung einfügen}. Eine Verbesserung dieses Verfahrens ermöglicht die Verwendung von Mehrstrahlern mit bis zu vier Antennen.

\subsubsection{Lidar}
\label{sec:Lidar}

Light Detection and Ranging, kurz Lidar, gehört zu den optischen Messverfahren und nutzt UV-, IR-Strahlen oder Strahlen aus dem sichtbaren Spektrum \cite{Winner.2015}. Für die Messung großer Entfernungen wird die Pulslaufzeitmessung genutzt bei dem kurze Lichtblitze hoher Leistung, meist Laser-Pulse, gesendet und die Laufzeit gemessen wird \cite{Trankler.2014}. Der Abstand wird ähnlich wie beim Ultraschall bestimmt:

\begin{equation}
d = \dfrac{c_L \Delta t}{2}
\label{eq:AbstandL}
\end{equation}

Mit der Lichtgeschwindigkeit $c_L$. Die Reichweite beim Lidar ist durch die Lichtintensität, welche den Laserschutzvorschriften genügen muss, beschränkt. Des Weiteren beeinflusst auch der Reflexionsgrad $\rho_L$ die Reichweite. Dieser ist insbesondere von der Oberfläche des Objektes abhängig, aber auch von seiner Größe. So ergibt sich für die reflektierte Lichtintensität $P_r$ für ein großes bzw. nahes Objekt die folgende Gleichung:

\begin{equation}
P_r = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^3 \cdot (Q_v/4)(\Phi/2)^2}
	\label{eq:Pgross}
\end{equation}

Für ein Objekt, das -- durch die Entfernung -- kleiner ist als der Lichtpunkt ändert sich Gleichung \ref{eq:Pgross} zu:

\begin{equation}
P_r = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^4 \cdot (Q_v Q_h/4)(\Phi/2)^2}. 
\label{eq:Pklein}
\end{equation}

Hierbei ist $\Phi$ der Winkel der Objektreflexion, $H$ die Objektbreite, $A_t$ die Empfangslinsenfläche, $T$ die Transmission der Atmosphäre, $Q_v$ und $Q_h$ die vertikale bzw. horizontale Strahldivergenz und $P_t$ die Laserleistung.

Das Sichtfeld kann von dem eindimensionalen Fall mit einem Strahl in nur eine Richtung auch horizontal und vertikal beliebig erweitert werden. Dafür gibt es zwei verschiedene Ansätze. Ein Ansatz ist der Einsatz eines schwenkbaren Spiegels, der den/die Laserstrahl/en ablenkt. Hierdurch ist ein horizontaler Öffnungswinkel von bis zu \unit{360}{°} und ein vertikaler Öffnungswinkel von bis zu \unit{120}{°} möglich. Der zweite Ansatz nutzt ein Array aus Laserdioden, die mittels Multiplexverfahren angesteuert werden. Der horizontale Öffnungswinkel bei dieser Variante beträgt bis zu \unit{110}{°} und der vertikale Öffnungswinkel ist von der Strahlbreite abhängig und beträgt in etwa \unit{2}{°} bis \unit{5}{°}.

\subsubsection{Kamera}
\label{sec:Kamera}
Die Kamera gehört zu den bildgebenden Sensoren und besitzt dadurch den Vorteil ähnliche Informationen wie das menschliche Auge zu produzieren. Somit können Objekte mit einer hohen Genauigkeit identifiziert werden. Jedoch ist die Entfernungsmessung mit einer Monokamera eher ungenau, da dies nur anhand der Auflösung geschätzt werden kann. Diese bestimmt auch den Sichtbereich und die Reichweite. Letzteres wird durch den Bereich des scharfen Abbildens begrenzt \cite{Hering.2016}. Die untere Grenze $a_v$ der Reichweite liegt vor und die obere Grenze $a_h$ hinter der Objektebene und ergeben sich mit

\begin{align}
a_v &= \dfrac{a f'^2}{f'^2 - u'k(a+f')}\\
a_h &= \dfrac{a f'^2}{f'^2 + u'k(a+f')}.
\label{eq:a_vh}
\end{align}

Dabei ist $k$ die Blendenzahl, $a$ der Abstand zwischen Objektebene und Eintrittspupille, $f'$ die Brennweite und $u'$ der Durchmesser des Unschärfekreises, der sich folgendermaßen bestimmen lässt:

\begin{equation}
u' = \dfrac{\text{Formatdiagonale}}{1000}
\label{eq:u'}
\end{equation}

Neben dem sichtbaren Spektrum können einige Kameras auch das Infrarotspektrum erkennen. So kann auch bei Nacht bzw. Dunkelheit die Kamera weiterhin eingesetzt werden. Es gibt zwei verschiedene Ansätze hierbei, die unterschiedliche Infrarotbereiche nutzen \cite{Winner.2015}. Eine Möglichkeit ist das Nahinfrarot (NIR). Hierbei wird die Szene mit NIR-Strahlung ausgeleuchtet, die von der Kamera erkannt wird. Die andere Möglichkeit ist der Einsatz von Ferninfrarot (FIR). In diesem Spektrum liegt die Wärmestrahlung von Objekten, die von speziellen Wärmebildkameras erfasst werden kann.

\subsection{Datenverarbeitung}
\label{sec:Datenverarbeitung}

Im folgenden Abschnitt wird auf die Datenverarbeitung genauer eingegangen. Hierzu gehört die Objekterkennung und das Objekttracking, welche durch eine Datenfusion erweitert werden können. Der Ablauf der Datenverarbeitung ist in Abbildung \ref{fig:Datenverarbeitung} schematisch dargestellt.

\begin{figure}[h]
	\centering
		\includegraphics[width=\textwidth,trim={3cm 8cm 3cm 7cm},clip]{pics/Datenverarbeitung.pdf}
	\caption{Schematischer Ablauf der Datenverarbeitung \cite{Winner.2015}}
	\label{fig:Datenverarbeitung}
\end{figure}

Als erstes wird die Signalaufnahme mittels des Sensors durchgeführt. Dem schließt sich sich eine Signalverarbeitung der Rohsignale an. Danach werden mit Hilfe einer Merkmalsextraktion Merkmals- bzw. Objekthypothesen aufgestellt. Fehlerquellen sind hierbei Artefakte durch Verletzung von physikalischen Annahmen in der Signalverarbeitung und Fehlinterpretationen durch Annahmen in der Merkmalsextraktion.

\subsubsection{Objekterkennung und Tracking}
\label{sec:Objekterkennung}
Für die Objekterkennung und das Tracking gibt es unterschiedliche Verfahren. Bei der Objekterkennung ist die Wahl des Verfahren insbesondere abhängig vom Sensortyp. \marginpar{Beispielbilder für Segmentierungsverfahren und Gradientenverfahren?}

Radar und Lidar erzeugen bei der Messung Punktwolken. Um in diesen Punktwolken Objekte zu identifizieren, werden Segmentierungsverfahren eingesetzt. Die Annahme bei diesem Verfahren ist, dass Messrohpunkte eines Objektes in enger Nachbarschaft liegen. So werden diese Punkte mittels Region-Growing oder Linienextraktion gruppiert bzw. verbunden. Nach diesem Schritt werden die Segmente in I- und L-Formen unterschieden. Mit hinzunehmen der Segmentabmessungen kann schließlich die Objektklasse bestimmt werden \cite{Walchshausl.2008}.

Bei der Kamera werden Gradientenverfahren und Matching Verfahren zur Merkmalsextraktion eingesetzt \cite{Winner.2015}. Die wichtigsten Merkmale beim Kamerabild sind Kanten und Ecken. Diese führen zu einer deutlichen Änderung des Bildsignals, welche mathematisch durch Gradienten beschrieben wird. Beim Matching Verfahren wird eine kleine Region um einen Bildpunkt herum mit den entsprechenden Punkten im nächsten Bild verglichen. Um nicht den gesamten Bildraum abzusuchen, wird das Verfahren auf Ecken und Kanten im Bild angewendet.

Für das Tracking werden insbesondere drei verschiedene Verfahren eingesetzt. Das sind der Bayes-Filter, der Kalman-Filter und der Partikelfilter \cite{Winner.2015}. Der Bayes-Filter ist ein allgemeingültiges Verfolgungsverfahren. Es schätzt aus der Beobachtung die neue mögliche Position. Die Beobachtung findet im Zustandsraum statt und gibt eine Wahrscheinlichkeitsdichte für den aktuellen Zustand heraus unter Berücksichtigung aller vorigen Beobachtungen.

Der Kalman-Filter schätzt die Zustände aufgrund von redundanten Daten. Die mögliche neue Position wird mit Systemeingangsdaten geschätzt und mit Messwerten verglichen. Die Differenz aus Schätzung und Messung wird schließlich gewichtet und dient als Korrektur des aktuellen Zustands.

Beim Partikelfilter wird die Wahscheinlichkeitsdicht durch die endliche Summe von Dircstößen mit Gewichten approximiert. Die Paare aus Gewicht und Zustand werden als Partikel betrachtet. Nach jedem Innovationsschritt werden schließlich die Gewichte aktualisiert.

\subsubsection{Sensordatenfusion}
\label{sec:Fusion}

Die Sensordatenfusion wird genutzt, um die Genauigkeit zu erhöhen bzw. mehr Informationen zu erhalten. Dies ist davon abhängig, welche Sensoren eingesetzt und wie sie eingesetzt werden. Im Allgemeinen wird in komplementär, konkurrierend und kooperativ unterschieden. Was das für den Sensoreinbau bedeutet ist in Abbildung \ref{fig:Fusionsansaetze} dargestellt. Werden Sensoren komplementär genutzt, ergänzen sich deren einzelne Sichtfelder zu einem großen. Sind Sensoren konkurrierend verbaut sind sie entweder redundant, d.h. es wird die gleiche Information generiert, oder konträr, d.h. es werden gegensätzliche Informationen erzeugt. Der letzte Ansatz ist der kooperative Einsatz von unterschiedlichen Sensoren, die zusammen einen höheren Informationsgehalt erzeugen \cite{Dietmayer2005}.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.6\textwidth,trim={3cm 6.5cm 9cm 4cm},clip]{pics/Fusionsansaetze.pdf}
	\caption{Ansätze der Datenaufnahme für die Datenfusion}
	\label{fig:Fusionsansaetze}
\end{figure}

Für die Sensordatenfusion gibt es zwei wesentliche Ansätze. Entweder werden die Daten implizit oder explizit fusioniert. Beispielhaft ist sind die Ansätze in den Abbildungen \ref{fig:ImpliziteFusion} und \ref{fig:ExpliziteFusion} dargestellt. Bei der impliziten Fusion werden die Sensordaten zeitlich nacheinander eingebracht. Dadurch wird eine zeitlich konsistente Datenverarbeitung nötig. Außerdem muss eine zeitliche Filterung durchgeführt werden, wenn die Messdaten vorliegen. Es wird jedoch schon eine Assoziation auf dem sensorspezifischen Abstraktionslevel durchgeführt, die bei der Fusion abgeglichen wird. Vorteilhaft bei der impliziten Fusion ist, dass keine Synchronisierung der Sensoren durchgeführt werden muss.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{pics/ImpliziteFusion.PNG}
	\caption{Implizite Sensordatenfusion mit asynchronen Sensoren \cite{Dietmayer2005}}
	\label{fig:ImpliziteFusion}
\end{figure}

Bei der expliziten Fusion wird abgewartet, bis alle Messdaten vorliegen und dann erst fusioniert. Somit findet eine zeitliche Filterung in einem festen Zeitraster statt und die Assoziation findet auf einem gemeinsamen Abstraktionslevel statt. Hierbei müssen die Messdaten jedoch synchronisierbar sein.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{pics/ExpliziteFusion.PNG}
	\caption{Explizite Sensordatenfusion mit synchronen Sensoren \cite{Dietmayer2005}}
	\label{fig:ExpliziteFusion}
\end{figure}

Die Datenfusion mit synchronen Sensoren führt zu einer sicheren und zuverlässigen Assoziation. Da jedoch der Sensor mit der längsten Akquisitionszeit den zeitlichen Versatz zwischen Messung und Assoziation bestimmt, ist der Algorithmus streng deterministisch und es kommt zu einem hohen Verzug zwischen Realwelt und Modell. Die Arbeit mit synchronisierten Sensoren bietet jedoch, neben der sicheren Assoziation, eine einfache Erweiterbarkeit um weitere Sensoren.

Werden asynchrone Sensoren bei der Datenfusion verwendet, müssen die Daten sequentiell eingebracht werden. Somit ist der Algorithmus nicht deterministisch. Durch dieses Vorgehen kann es jedoch zu Quantisierungsfehlern kommen. Insbesondere, wenn die Daten des Sensors mit der kürzesten Latenz kurz nach denen vom Sensor mit der längsten Latenz eingebracht werden. Dann können diese Informationen nicht mehr in die Assoziation mitberücksichtigt werden. Je ähnlicher die Latenzzeiten der einzelnen Sensoren ist, desto geringer wird schließlich auch der Fehler der Schätzung. 

\subsection{Sensoreinsatz im Fahrzeug}
\label{sec:KFZSensor}

\begin{table}
	\centering
		\begin{tabularx}{\textwidth}{p{2cm}p{2.5cm}Xc}
		\textbf{Quelle} & \textbf{Sensorsetup} & \textbf{Beschreibung} &  \textbf{Jahr}\\ \toprule
		\cite{RudiLindl.2009} & Radar\newline Lidar\newline Kamera & Sensorfusion für Fahrerassistenzsysteme mit hohen Ansprüchen  & 2009\\ \midrule
\cite{Perrone.2010} & Stereokamera & Modell zur Datenanalyse von Stereo Kameras  & 2010 \\ \midrule
\cite{Ahtiainen.2010} & 2 x Radar & Erkennen eines Menschen mit Radar  & 2010\\ \midrule
\cite{Bartsch.2012} & Radar & Erkennen eines Menschen mit Radar & 2012\\ \midrule
\cite{Krzikalla2013} & Lidar\newline GPS & Selbstlokalisierung mit Hilfe von GPS, Laserscanner und digitaler Karte &  2013\\ \midrule
\cite{Yalcin.2013} & Lidar & Positionierung des LIDAR, Fahrbahnbegrenzung erkennen, Objekterkennung  & 2013\\ \midrule
\cite{Apatean.2013} & IR\newline VIS Video & Fusioniert Infrarotkameradaten mit Daten des sichtbaren Spektrums einer Kamera  & 2013\\ \midrule
\cite{Schindler.} & Kamera\newline Lidar\newline GPS & Erzeugen einer Karte mit Hilfe von aktuellen Messdaten, Selbstlokalisierung des Fahrzeugs  & 2013\\ \midrule
\cite{Lundgren.} & GPS\newline Gyroscope\newline Kamera\newline Radar & KFZ ausgestattet mit GPS, Gyroscope, Geschwindigkeitsmesser, Kamera, Radar zur Selbstlokalisierung  & 2014\\ \midrule
\cite{Negied.2015} & IR & Literaturauflistung bzgl Erkennung von Menschen mit Infrarotsensor & 2015\\ \midrule
\cite{Wang.2015}   & FIR & stellt einen Filter zur Objekterkennung mit Infrarot vor &  2015\\ \midrule
\cite{Vivacqua.2017} & GPS\newline Gyroscope\newline Kamera & KFZ ausgestattet mit GPS, Gyroscope, Kamera und Laptop zur Selbstlokalisierung & 2017\\ \bottomrule
		\end{tabularx}
	\caption{Einsatz von Sensoren im Fahrzeug}
	\label{tab:LitFahrzeug}
\end{table}

Ultraschall
\begin{itemize}
	\item Einpark-Assistent
	\item Front/Heck
\end{itemize}

Radar
\begin{itemize}
	\item ACC, ACC S\&G
	\item Spurwechselassistent
	\item Toter Winkel Assistent
	\item Front/Heck
\end{itemize}

Lidar
\begin{itemize}
	\item ACC
	\item Front/360°
\end{itemize}

Kamera
\begin{itemize}
	\item Fahrbahnerkennung
	\item Objekterkennung
	\item Verkehrsschilderkennung
	\item Spurhalteassistent
	\item Einparkassistent
	\item Front/Heck
	\item sichtbar/IR Spektrum
	\item Fusion mit Radar/Lidar
\end{itemize}

\begin{itemize}
	\item Fahrerassistenzsysteme
	\item autonomes Fahren
\end{itemize}

\subsection{Sensoreinsatz in der Infrastruktur}
\label{sec:InfraSensor}

\begin{itemize}
	\item Kamera
	\item Radar
	\item Lidar
	\item Unfallforschung
\end{itemize}

\begin{table}
	\centering
		\begin{tabularx}{\textwidth}{p{2cm}p{2.5cm}Xc}
		\textbf{Quelle} & \textbf{Sensorsetup} & \textbf{Beschreibung} &  \textbf{Jahr}\\ \toprule
			\cite{J.Ehrlichetal..2009} & Kamera\newline Lidar \newline IR & CCTV, Laserscanner, IR zur Verkehrs- und Umweltbeobachtung (Bestimmung der Witterung, Tageszeit) &  2009\\ \midrule
\cite{Meissner.} & Lidar & Einsatz und Modellierung von Laserscannern an Kreuzungen &  2012\\ \midrule
\cite{Goldhammer2012} & Lidar\newline Kamera & Beschreibt den Versuchsaufbau an einer Kreuzung zur Beobachtung des Verkehrs &  2012\\ \midrule
\cite{Hospedales.2012} & Kamera & Vergleicht Algorithmen zur Verhaltenserkennung von Verkehrsobjekten in Kameradaten &  2012\\ \midrule
\cite{Goldhammer.18.09.2013} & Lidar\newline Kamera & Einsatz von Sensoren an einer Kreuzung (Anbauorte) &  2013\\ \midrule
\cite{EliasStrigel2013} & Lidar\newline Kamera & Einsatz von Kameras und Laserscanner an einer Kreuzung zur Beobachtung des Verkehrs  & 2013\\ \midrule
\cite{Strigel.} & Lidar\newline Kamera & Beschreibt den Versuchsaufbau an einer Kreuzung zur Beobachtung des Verkehrs & 2014\\ \midrule
\cite{Jodoin.} & Kamera & Stellt einen Algorithmus zum Objekttracking für die Bildverarbeitung vor, der an unterschiedlichen Kreuzungen getestet wurde &  2014\\ \midrule
\cite{DatondjiSokemiReneEmmanuel.2016} & Kamera & Listet und diskutiert Ansätze zur Verkehrsbeobachtung an Kreuzungen &  2016\\ \midrule
\cite{KnakeLanghorst.2016} & Kamera\newline IR\newline Radar & Stellt die Forschungsplatform AIM zur Untersuchung des Verkehrs vor &  2016\\ \midrule
\cite{Shirazi.2017} & GPS\newline Radar\newline Lidar\newline Kamera & Vergleicht verschiedene Ansätze der Verkehrsbeobachtung und Analyse & 2016\\ \midrule
\cite{Dotzauer2017} & Kamera\newline IR\newline Radar & Untersuchung von Konflikten zwischen Fahrradfahrern und motorisierten Fahrzeugen &  2017\\\midrule
\cite{Garcia2018} & Radar & Stellt \unit{76-81}{GHz} Radar zur Verkehrsüberwachung vor &  2018\\ \bottomrule
		\end{tabularx}
	\caption{Einsatz von Sensoren in der Infrastruktur}
	\label{tab:LitInfrastruktur}
\end{table}