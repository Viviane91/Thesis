\section{Umfelderfassung}
\label{sec:TechnikStand}
Dieses Kapitel stellt zunächst die bestehenden Sensoren zur Umfelderfassung vor und erläutert deren Funktionsprinzipien. Anschließend wird auf die Sensordatenverarbeitung eingangen. Dazu gehören die Datenfusion, die Objekterkennung und das Tracking. Auf diesen Grundlagen aufbauend wird in den Kapiteln \ref{sec:KFZSensor} und \ref{sec:InfraSensor} aufgezeigt, wie dies im Fahrzeug und auch in der Infrastruktur genutzt wird.

\subsection{Sensoren zur Umfelderfassung}
\label{sec:Sensoren}
Die Sensoren zur Umfelderfassung werden in entfernungsgebende und bildgebende Sensoren unterschieden. Zu ersterem gehören der Ultraschall, der Radar und der Lidar. Zu letzterem die Kamera mit dem sichtbaren und dem Infrarotspektrum. Im Folgendem werden die einzelnen Sensorprinzipien erläutert.

\subsubsection{Ultraschall}
\label{sec:Ultraschall}

Als Ultraschall werden die Schallfrequenzen ab \unit{20}{kHz} bezeichnet. Sie gehören zu den Frequenzen, die für das menschliche Ohr nicht hörbar sind. Die Messung mit Ultraschall gehört zu den Laufzeitmessungen. Ein Sender emittiert Schallwellen, die schließlich von Objekten reflektiert werden. Mit Hilfe der gemessenen Laufzeit $\Delta t$ bis das Echo wieder am Sender ankommt kann der Abstand $d$ zum gemessenen Objekt bestimmt werden \cite{Trankler.2014}:

\begin{equation}
d = \dfrac{c_S \Delta t}{2}.
\label{eq:AbstandU}
\end{equation}

Mit der Schallgeschwindigkeit $c_S$. Da die Strecke zwischen Sender und Objekt zweimal durchlaufen wird, muss diese halbiert werden um den tatsächlichen Abstand zu erhalten. Neben der reinen Abstandsmessung kann mit Hilfe von zwei Sendern, deren Erfassungsbereiche überlappen, auch eine Positionsbestimmung durchgeführt werden. Hierzu wird das Trilaterationsverfahren genutzt. Für ein rundes Objekt ist dies in Abbildung \ref{fig:Trilateration} dargestellt.\marginpar{Abbildung überarbeiten!} Der Abstand $D$ berechnet sich mittels des Satzes von Pythagoras:

\begin{equation}
D = \sqrt{DE1^2 - \dfrac{\left(d^2 + DE1^2-DE2^2\right)^2}{4d^2}}
\label{eq:PosD}
\end{equation}

\begin{figure}%
\centering
\includegraphics[width=0.6\textwidth,trim={2cm 11cm 16cm 3.5cm},clip]{pics/Position_Ultraschall.pdf}%
\caption{Veranschaulichung des Trilaterationsprinzips für ein rundes Objekt}%
\label{fig:Trilateration}%
\end{figure}

Die Reichweite des Ultraschallsensors ist abhängig von der ausgesendeten Schallintensität $I_S$, da die Schallintensität in Abhängigkeit von der Entfernung $r$ des gemessenen Objektes abnimmt. Somit ergibt sich, mit der effektiven Reflexionsfläche $\sigma$ und bezogen auf den Normabstand $r_1$, die reflektierte Schallintensität 

\begin{equation}
I_{refl}=\sigma I_s \left(\dfrac{r_1}{2r}\right)^2.
\label{eq:Irefl}
\end{equation}

Des Weiteren verringert der Reflexionsgrad $\rho_S$ und die Impedanz der Atmosphäre die Schallintensität bei der Reflexion. Als untere Grenze zur Objektmessung muss die Schallintensität des Empfangssignals oberhalb des Rauschens liegen, d.h. \unit{$\geq$10}{dB} sein.

Zur Schallerzeugung und -empfang wird bei Ultraschallsensoren eine Membran aus einer Piezokeramik eingesetzt \cite{Winner.2015}. Zum Aussenden der Schallwellen wird die Membran aktiv in Schwingung versetzt und nach einer festgelegten Sendedauer wieder zur Ruhe gebracht. Die Zeit bis der reflektierte Schall die Membran wieder zur Schwingung anregt wird mit Gleichung \ref{eq:AbstandU} zur Abstandsbestimmung genutzt.

\subsubsection{Radar}
\label{sec:Radar}
radio detection and ranging\\
elektromagnetische Wellen im Radiofrequenzbereich/Mikrowellen\\
24GHz und 77GHz für Automobil\\
Strahlen gebündelt mit Richtantenne\\
Zwei Messverfahren: Puls-Doppler und Dauerstrich (FMCW)\\
Puls-Doppler: Rechteckpulse mit Zwischenfrequenz (Mischer), Abstand mit Pulslaufzeit bezogen auf Pulsmitte, betrachtet Doppler-Frequenzen, Annäherung positiv/Entfernen negativ\\
FMCW: Frequenzrampen, Differenzfrequenz betrachten (je größer $\Delta f$ desto größer $\Delta t$ desto größer d)\\
Reichweite von Sendeleistung und Richtcharakteristik abhängig\\
Rückstreuquerschnitt $\sigma$ beeinflusst reflektiertes Signal\\
je größer $\sigma$ desto besser erkennbar\\
FoV erweiterbar durch Scan oder Mehrstrahler\\

Die Radarmessung (Radio Detection And Ranging) gehört zu den berührungslosen Messverfahren und wird insbesondere bei anspruchsvollen Umgebungsbedingungen eingesetzt \cite{Trankler.2014}. Bei diesem Verfahren werden elektromagnetische Wellen im Mikrowellenbereich eingesetzt, welche kaum anfällig gegenüber Temperaturschwankungen und Nebel sind. Für den Automobilbereich sind die \unit{24}{GHz} und \unit{77}{GHz} Frequenzen reserviert \cite{Winner.2015}.

Zur Abstandsmessung finden zwei verschiedene Ansätze Verwendung. Das ist zum Einen das Puls-Doppler-Verfahren und zum Anderen das Dauerstrich-Verfahren (FMCW). Ersteres funktioniert ähnlich wie die Laufzeitmessung beim Ultraschall. 

\begin{align}
	r &= \dfrac{c}{2} \cdot \dfrac{\Delta t_0 \omega_{obj} - \varphi_{BA}}{m_{\omega} \Delta t_0 - \Delta \omega_{BA}},\\
	\dot{r} &= \dfrac{c}{2 \omega_0} \cdot \dfrac{m_{\omega} \Delta \varphi_{BA} - \Delta \omega_{BA} \omega_{obj}}{m_{\omega} \Delta t_0 - \Delta \omega_{BA}}
\end{align}


Radar steht für Radio Detection And Ranging und sendet elektromagnetischen Wellen im Radiofrequenzbereich aus. Für den Automobilbereich sind die \unit{24}{GHz} und \unit{77}{GHz} Frequenzbänder reserviert \cite{Winner.2015}.

Im Gegensatz zum Ultraschall breiten sich beim Radar die Wellen nicht gleichmäßig in alle Raumrichtungen aus, sondern werden mit Hilfe einer sogenannten Richtantenne gebündelt. Je nach Richtcharakteristik ergibt sich der Antennengewinn $G$, der Einfluss auf die Reichweite nimmt. Die Empfangsleistung für ein reflektiertes Radarsignal ergibt sich zu

\begin{equation}
P_R = \dfrac{10^{-2kr/1000}  \sigma  \lambda^2  G^2  V_{mp}^2  P_{total}}{(4\pi)^3 r^4}
\label{eq:Empfangsleistung}
\end{equation}

mit dem Rückstreuquerschnitt

\begin{equation}
\sigma_{plate} = 4\pi \dfrac{A^2}{\lambda^2}.
\label{eq:Rueckstreuquerschnitt}
\end{equation}

Gleichung \ref{eq:Empfangsleistung} berücksichtigt außerdem sogenannte Signalleistungsschüttler mit dem Faktor $V_{mp}^2$, $0 \leq V_{mp} \leq 2$.

Bei der Abstandsmessung wird neben der Laufzeitbestimmung der Doppler-Effekt genutzt. Der Doppler-Effekt besagt, dass sich die Frequenz bei der Reflexion in Abhängigkeit von der Änderung des Abstandes $\dot{r}$ ändert. Diese Frequenz wird auch Dopplerfrequenz $f_{Doppler}$ genannt und ergibt sich mit der Trägerfrequenz $f_0$ und der Lichtgeschwindigkeit $c$ folgendermaßen:

\begin{equation}
f_{Doppler} = - 2 \dot{r} f_0/c
\label{eq:Doppler}
\end{equation}

Bei einer Annäherung ($\dot{r}<0$) ist diese positiv und beim Entfernen negativ. Stehende Objekte können mit diesem Effekt jedoch nicht gemessen werden.

\subsubsection{Lidar}
\label{sec:Lidar}

Light Detection and Ranging, kurz Lidar, gehört zu den optischen Messverfahren und nutzt UV-, IR-Strahlen oder Strahlen aus dem sichtbaren Spektrum \cite{Winner.2015}. Für die Messung großer Entfernungen wird die Pulslaufzeitmessung genutzt bei dem kurze Lichtblitze hoher Leistung, meist Laser-Pulse, gesendet und die Laufzeit gemessen wird \cite{Trankler.2014}. Der Abstand wird ähnlich wie beim Ultraschall bestimmt:

\begin{equation}
d = \dfrac{c_L \Delta t}{2}
\label{eq:AbstandL}
\end{equation}

Mit der Lichtgeschwindigkeit $c_L$. Die Reichweite beim Lidar ist durch die Lichtintensität, welche den Laserschutzvorschriften genügen muss, beschränkt. Des Weiteren beeinflusst auch der Reflexionsgrad $\rho_L$ die Reichweite. Dieser ist insbesondere von der Oberfläche des Objektes abhängig, aber auch von seiner Größe. So ergibt sich für die reflektierte Lichtintensität $P_r$ für ein großes bzw. nahes Objekt die folgende Gleichung:

\begin{equation}
P_r = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^3 \cdot (Q_v/4)(\Phi/2)^2}
	\label{eq:Pgross}
\end{equation}

Für ein Objekt, das -- durch die Entfernung -- kleiner ist als der Lichtpunkt ändert sich Gleichung \ref{eq:Pgross} zu:

\begin{equation}
P_r = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^4 \cdot (Q_v Q_h/4)(\Phi/2)^2}. 
\label{eq:Pklein}
\end{equation}

Hierbei ist $\Phi$ der Winkel der Objektreflexion, $H$ die Objektbreite, $A_t$ die Empfangslinsenfläche, $T$ die Transmission der Atmosphäre, $Q_v$ und $Q_h$ die vertikale bzw. horizontale Strahldivergenz und $P_t$ die Laserleistung.

Das Sichtfeld kann von dem eindimensionalen Fall mit einem Strahl in nur eine Richtung auch horizontal und vertikal beliebig erweitert werden. Dafür gibt es zwei verschiedene Ansätze. Ein Ansatz ist der Einsatz eines schwenkbaren Spiegels, der den/die Laserstrahl/en ablenkt. Hierdurch ist ein horizontaler Öffnungswinkel von bis zu \unit{360}{°} und ein vertikaler Öffnungswinkel von bis zu \unit{120}{°} möglich. Der zweite Ansatz nutzt ein Array aus Laserdioden, die mittels Multiplexverfahren angesteuert werden. Der horizontale Öffnungswinkel bei dieser Variante beträgt bis zu \unit{110}{°} und der vertikale Öffnungswinkel ist von der Strahlbreite abhängig und beträgt in etwa \unit{2}{°} bis \unit{5}{°}.

\subsubsection{Kamera}
\label{sec:Kamera}
Die Kamera gehört zu den bildgebenden Sensoren und besitzt dadurch den Vorteil ähnliche Informationen wie das menschliche Auge zu produzieren. Somit können Objekte mit einer hohen Genauigkeit identifiziert werden. Jedoch ist die Entfernungsmessung mit einer Monokamera eher ungenau, da dies nur anhand der Auflösung geschätzt werden kann. Diese bestimmt auch den Sichtbereich und die Reichweite. Letzteres wird durch den Bereich des scharfen Abbildens begrenzt \cite{Hering.2016}. Die untere Grenze $a_v$ der Reichweite liegt vor und die obere Grenze $a_h$ hinter der Objektebene und ergeben sich mit

\begin{align}
a_v &= \dfrac{a f'^2}{f'^2 - u'k(a+f')}\\
a_h &= \dfrac{a f'^2}{f'^2 + u'k(a+f')}.
\label{eq:a_vh}
\end{align}

Dabei ist $k$ die Blendenzahl, $a$ der Abstand zwischen Objektebene und Eintrittspupille, $f'$ die Brennweite und $u'$ der Durchmesser des Unschärfekreises, der sich folgendermaßen bestimmen lässt:

\begin{equation}
u' = \dfrac{\text{Formatdiagonale}}{1000}
\label{eq:u'}
\end{equation}

Neben dem sichtbaren Spektrum können einige Kameras auch das Infrarotspektrum erkennen. So kann auch bei Nacht bzw. Dunkelheit die Kamera weiterhin eingesetzt werden. Es gibt zwei verschiedene Ansätze hierbei, die unterschiedliche Infrarotbereiche nutzen \cite{Winner.2015}. Eine Möglichkeit ist das Nahinfrarot (NIR). Hierbei wird die Szene mit NIR-Strahlung ausgeleuchtet, die von der Kamera erkannt wird. Die andere Möglichkeit ist der Einsatz von Ferninfrarot (FIR). In diesem Spektrum liegt die Wärmestrahlung von Objekten, die von speziellen Wärmebildkameras erfasst werden kann.

\subsection{Datenverarbeitung}
\label{sec:Datenverarbeitung}

Im folgenden Abschnitt wird auf die Datenverarbeitung genauer eingegangen. Hierzu gehört die Objekterkennung und das Objekttracking, welche durch eine Datenfusion erweitert werden können. Der Ablauf der Datenverarbeitung ist in Abbildung \ref{fig:Datenverarbeitung} schematisch dargestellt.

\begin{figure}[h]
	\centering
		\includegraphics[width=\textwidth,trim={3cm 8cm 3cm 7cm},clip]{pics/Datenverarbeitung.pdf}
	\caption{Schematischer Ablauf der Datenverarbeitung \cite{Winner.2015}}
	\label{fig:Datenverarbeitung}
\end{figure}

Als erstes wird die Signalaufnahme mittels des Sensors durchgeführt. Dem schließt sich sich eine Signalverarbeitung der Rohsignale an. Danach werden mit Hilfe einer Merkmalsextraktion Merkmals- bzw. Objekthypothesen aufgestellt. Fehlerquellen sind hierbei Artefakte durch Verletzung von physikalischen Annahmen in der Signalverarbeitung und Fehlinterpretationen durch Annahmen in der Merkmalsextraktion.

\subsubsection{Objekterkennung und Tracking}
\label{sec:Objekterkennung}
Segmentierungsverfahren (Lidar und Radar):\\
Messrohpunkte eines Objektes liegen in enger Nachbarschaft\\
Verbinden/Gruppieren der Punkte mittels Region-Growing oder Linienextraktion\\
Einteilen der Segmente in I- und L-Formen\\
Segmentabmessungen bestimmen Objektklasse

''U''-Form bzw parallele Kanten bei Kamera:\\
Kantenpaare suchen und um eine horizontale Kante erweitern (Kopf)\\

Gradientenhistogramme als Bildmerkmale:\\
Histogramm über Häufigkeit der vorkommenden Farbwerte im Bild\\
Mit Kantenextraktion bis auf Fußgänger in Video\\
Merkmalsrekonstruktion durch Achsspiegelung bei teilweise verdeckten Objekten\\
\cite{Walchshausl.2008}

Bayes-Filter:\\
allgemeingültiges Verfolgungsverfahren\\
schätzt aus der Beobachtung neue mögliche Position\\
Beobachtung im Zustandsraum\\
gibt Wahrscheinlichkeitsdichte für aktuellen Zustand mit Berücksichtigung aller vorigen Beobachtungen\\

Kalman-Filter (Tracking und Fusion):\\
Zustände aufgrund von redundanten Daten schätzen\\
schätzt aufgrund von Systemeingangsdaten mögliche neue Position\\
vergleicht Schätzung mit Messwerten\\
Differenz wird gewichtet und dient als Korrektur des aktuellen Zustands\\

Partikelfilter für Tracking\\
approximiert Wahrscheinlichkeit durch endliche Summe von Diracstößen mit Gewichten\\
Die Paare aus Gewicht und Zustand sind Partikel\\
Gewichte werden im Innovationsschritt aktualisiert\\
\cite{Winner.2015}
\cite{RudiLindl.2009}


\subsubsection{Datenfusion}
\label{sec:Fusion}

\begin{itemize}
			\item Datenaufnahme (Komplementär, Konkurrierend, Kooperativ)
			\item implizit/explizit
			\item a-/synchron
		\end{itemize}\cite{Dietmayer2005}

\subsection{Sensoreinsatz im Fahrzeug}
\label{sec:KFZSensor}

Ultraschall
\begin{itemize}
	\item Einpark-Assistent
	\item Front/Heck
\end{itemize}

Radar
\begin{itemize}
	\item ACC, ACC S\&G
	\item Spurwechselassistent
	\item Toter Winkel Assistent
	\item Front/Heck
\end{itemize}

Lidar
\begin{itemize}
	\item ACC
	\item Front/360°
\end{itemize}

Kamera
\begin{itemize}
	\item Fahrbahnerkennung
	\item Objekterkennung
	\item Verkehrsschilderkennung
	\item Spurhalteassistent
	\item Einparkassistent
	\item Front/Heck
	\item sichtbar/IR Spektrum
	\item Fusion mit Radar/Lidar
\end{itemize}

\begin{itemize}
	\item Fahrerassistenzsysteme
	\item autonomes Fahren
\end{itemize}

\subsection{Sensoreinsatz in der Infrastruktur}
\label{sec:InfraSensor}

\begin{itemize}
	\item Kamera
	\item Radar
	\item Lidar
	\item Unfallforschung
\end{itemize}