\section{Grundlagen}
\label{chap:Grundlagen}

Dieses Kapitel definiert zunächst den Begriff der "`Umgebung"'. Anschließend werden Sensoren zur Umfelderfassung vorgestellt und deren Funktionsprinzipien erläutert. Abschließend wird auf die Sensordatenverarbeitung eingegangen. Dazu gehören die Datenfusion, die Objekterkennung und das Tracking. 
%Auf diesen Grundlagen aufbauend werden in den Abschnitten \ref{sec:KFZSensor} und \ref{sec:InfraSensor} Anwendungen im Fahrzeug und in der Infrastruktur erläutert.
%\subsection{Umfeld}
%\label{sec:Umfeld}
%Definition Umfeld\\
%Unterscheidung in statisches und dynamisches Umfeld\\
%statisch: Straßenführung, Ampeln, Brücken, Baustellen, Verkehrsschilder...\\
%dynamisch: Verkehrsteilnehmer (KFZ, LKW, Fußgänger, Zweiräder...)

\subsection{Begriffserklärung "`Umfeld"'}
\label{sec:Umfeld}
Bei der Umgebung wird zwischen dem statischen und dem dynamischen Umfeld unterschieden. Zu ersterem gehören unter anderem der Verlauf der Fahrstreifen, Kreuzungen und statische Objekte wie Baustellen, Verkehrsschilder oder Brücken. Alle dynamischen Objekte wie \acp{Lkw}, \acp{Pkw}, Fahrradfahrer und Fußgänger gehören zum dynamischen Umfeld. Auch der Status von Lichtsignalanlagen oder die Witterung gehören hierzu.

\subsection{Sensorik zur Umfelderfassung}
\label{sec:Sensoren}
Die Sensoren zur Umfelderfassung werden in entfernungsgebende und bildgebende Sensoren unterschieden. Zu ersterem gehören der Ultraschall, das Radar und das Lidar. Zu letzterem zählt die Kamera mit dem sichtbaren und dem Infrarotspektrum. Im Folgenden werden die einzelnen Sensorprinzipien erläutert.

\subsubsection{Ultraschall}
\label{sec:Ultraschall}

Als Ultraschall werden die Schallfrequenzen zwischen \unit[20]{kHz} und \unit[10]{GHz} bezeichnet \cite{Hering.2016}. Sie sind für das menschliche Ohr nicht hörbar. Die Messung mit Ultraschall gehört zu den Laufzeitmessungen. Ein Sender emittiert Schallwellen, die von Objekten reflektiert werden. Zur Schallerzeugung und -empfang wird bei Ultraschallsensoren eine Membran aus einer Piezokeramik eingesetzt \cite{Winner.2015}. Zum Aussenden der Schallwellen wird die Membran aktiv in Schwingung versetzt und nach einer festgelegten Sendedauer wieder zur Ruhe gebracht. Die Laufzeit $\Delta t$, bis der reflektierte Schall die Membran wieder zur Schwingung anregt, wird gemessen. Diese wird zusammen mit der Schallgeschwindigkeit $c_S$ genutzt, um den Abstand $D$ zu bestimmen \cite{Trankler.2014}:
\begin{equation}
D = \dfrac{c_S}{2} \Delta t.
\label{eq:AbstandU}
\end{equation}

Da die Strecke zwischen Sender und Objekt zweimal durchlaufen wird, muss sie halbiert werden um den tatsächlichen Abstand zu erhalten. Um die Objektposition zu bestimmen, werden zwei Sensoren, deren Erfassungsbereiche überlappen, benötigt. Dieses Verfahren wird Trilaterationsverfahren genannt \cite{Winner.2015}. Abbildung\,\ref{fig:Trilateration} zeigt den Ansatz für ein rundes Objekt. 

\begin{figure}[hbtp]%
\centering
\includegraphics[width=0.6\textwidth,trim={2cm 11cm 16cm 3.5cm},clip]{pics/Position_Ultraschall.pdf}%
\caption{Veranschaulichung des Trilaterationsprinzips für ein rundes Objekt}%
\label{fig:Trilateration}%
\end{figure}

Der Abstand $D$ berechnet sich mittels der Direktechos DE, dem Senderabstand $s$ und des Satzes von Pythagoras:
\begin{equation}
D = \sqrt{\text{DE1}^2 - \dfrac{\left(s^2 + \text{DE1}^2-\text{DE2}^2\right)^2}{4s^2}}
\label{eq:PosD}
\end{equation}

Mit Hilfe des Kreuzechos kann des Weiteren bestimmt werden, ob es sich um ein rundes Objekt handelt oder um eine Wand. Als Kreuzecho wird der Empfang der Schallwellen eines anderen Sensors bezeichnet. Für ein rundes Objekt ergibt sich das Kreuzecho KE zu
\begin{equation}
\text{KE}_{rund} = \dfrac{\text{DE1} + \text{DE2}}{2}
\label{eq:KE_rund}
\end{equation}

und für eine Wand zu
\begin{equation}
\text{KE}_{Wand} = \sqrt{\dfrac{s^2}{4} + \text{DE1} \times \text{DE2}}.
\label{eq:KE_Wand}
\end{equation}

Die Reichweite des Ultraschallsensors ist abhängig von der ausgesendeten Schallintensität $I_S$, die in Abhängigkeit vom Abstand $D$ des gemessenen Objektes abnimmt. Des Weiteren verringern der Reflexionsgrad $\rho_S$ des Objektes und die Impedanz der Atmosphäre die Schallintensität bei der Reflexion. Als untere Grenze zur Objektmessung muss die Schallintensität des Empfangssignals oberhalb des Messrauschens liegen, das heißt es muss \unit{$\geq$10}{dB} sein \cite{Trankler.2014}.

Ultraschallsensoren besitzen eine geringe Witterungsabhängigkeit. Jedoch reagiert die Membran empfindlich auf Verschmutzung und Eis.

\subsubsection{Radar}
\label{sec:Radar}

Die Radarmessung (Radio Detection And Ranging) gehört zu den entfernungsgebenden Messverfahren und wird insbesondere bei anspruchsvollen Umgebungsbedingungen eingesetzt \cite{Trankler.2014}. Bei diesem Verfahren werden elektromagnetische Wellen im Mikrowellenbereich eingesetzt, welche kaum anfällig gegenüber Temperaturschwankungen und Nebel bzw. Regen sind. Für den Automobilbereich sind die Frequenzen \unit[24]{GHz} und \unit[77]{GHz} reserviert \cite{Winner.2015}. Das \unit[24]{GHz}-Band wird für das \ac{SRR} genutzt und das \unit[77]{GHz}-Band für das \ac{LRR}.

Zur Abstands- und Geschwindigkeitsmessung finden zwei verschiedene Ansätze Verwendung, die sich in der Frequenzmodulation unterscheiden: das Dauerstrichradar (\ac{FMCW}) und die Chirp Frequence Modulation. Die Frequenzverläufe der beiden Modulationsverfahren sind in Abbildung\,\ref{fig:RadarFrequenzverlauf} dargestellt.

\begin{figure}[hbtp]%
\centering
\subfigure[\label{fig:RadarFMCW}][FMCW]{\includegraphics[page=1,width=0.48\textwidth,trim={4cm 6.4cm 8cm 3cm},clip]{pics/Radarsignale.pdf}}
    \subfigure[\label{fig:RadarChirp}][Chirp Frequence Modulation]{\includegraphics[page=2,width=0.48\textwidth,trim={4cm 6.4cm 8cm 3cm},clip]{pics/RadarSignale.pdf}}
\caption{Beispielhafter Frequenzverlauf der Signalerzeugung vom Radar \cite{Winner.2015}}%
\label{fig:RadarFrequenzverlauf}
\end{figure}

Das Dauerstrichradar erzeugt durch die kontinuierliche und rampenförmige Veränderung der Momentanfrequenz mit der Treppensteigung $m_{\omega}$ eine konstante Phasenverschiebung von $+(2D/c_L)^2m_{\omega}$. Der Abstand $D$ und die Geschwindigkeit $\dot{D}$ wird bei diesem Verfahren mithilfe der Frequenzverschiebung bestimmt:
\begin{align}
	D &= \dfrac{c_L}{2} \cdot \dfrac{\omega_{obj,1} - \omega_{obj,2}}{m_{\omega,1} - m_{\omega,2}},\\
	\dot{D} &= \dfrac{c_L}{2 \omega_0} \cdot \dfrac{m_{\omega,1}\omega_{obj,1}-m_{\omega,2}\omega_{obj,2}}{m_{\omega,1} - m_{\omega,2}}.
	\end{align}

Bei der Chirp Frequence Modulation wird ein Sägezahnsignal mit einem Hub von $f_{chirp}=30...300\,\text{MHz}$ erzeugt. Die Dopplerfrequenz $f_{Doppler}$ bestimmt hierbei die Wiederholrate und sollte etwa \unit[80]{kHz} betragen, um Mehrdeutigkeiten zu vermeiden. Der Abstand $D$ wird mit dem Puls-Doppler-Verfahren mit der Laufzeit $\Delta t$, bezogen auf die Pulsmitte, und der Lichtgeschwindigkeit $c_L$ bestimmt. Für die Geschwindigkeit $\dot{D}$ wird die Dopplerfrequenz $f_{Doppler}$ und die Trägerfrequenz $f_0$ genutzt.
\begin{align}
	D &= \dfrac{1}{2}c_L \Delta t\\
	\dot{D} &= - \dfrac{c_L}{2} \dfrac{f_{Doppler}}{f_0}
\end{align}

Der Doppler-Effekt besagt, dass sich die Frequenz bei der Reflexion in Abhängigkeit von der Änderung des Abstandes $\dot{D}$ ändert. Diese Frequenz wird Dopplerfrequenz $f_{Doppler}$ genannt. Beim Annähern ($\dot{D}<0$) ist diese Frequenz positiv und beim Entfernen negativ.

Die Reichweite des Radars ist abhängig von der Sendeleistung und der Richtcharakteristik der Antenne \cite{Winner.2015}. Je nach Richtcharakteristik verändert sich der Antennengewinn, der Einfluss auf die Reichweite nimmt. 
%So ergibt sich die Empfangsleistung für ein reflektiertes Radarsignal zu
%\begin{equation}
%P_R = \dfrac{10^{-2kr/1000}  \sigma  \lambda^2  G^2  V_{mp}^2  P_{total}}{(4\pi)^3 r^4}
%\label{eq:Empfangsleistung}
%\end{equation}

%mit dem Rückstreuquerschnitt des Objektes
%\begin{equation}
%\sigma_{plate} = 4\pi \dfrac{A^2}{\lambda^2}.
%\label{eq:Rueckstreuquerschnitt}
%\end{equation}

%Gleichung \ref{eq:Empfangsleistung} berücksichtigt außerdem sogenannte Signalleistungsschüttler mit dem Faktor $V_{mp}^2$, $0 \leq V_{mp} \leq 2$.

Für die Winkelbestimmung kommen verschiedene Verfahren zum Einsatz. Ein Verfahren ist das mechanische Scanning, bei dem eine Strahlablenkeinheit oder eine Planarantenne mechanisch geschwenkt wird. Dabei rotiert die Radarkeule mit einer Schrittweite von etwa \unit[1]{°}, siehe Abbildung\,\ref{fig:RadarScan}.

Ein weiteres Verfahren ist das Monopuls-Verfahren. Hierbei erzeugt eine separate Antenne einen Sendestrahl, der von einer Doppelantennen-Anordnung empfangen wird, siehe Abbildung\,\ref{fig:RadarMonopuls}. Eine Verbesserung dieses Verfahrens wird durch die Verwendung von Mehrstrahlern mit bis zu vier Antennen ermöglicht. Eine Konzeptvariante dieser zwei Verfahren ist das Dual-Sensor-Konzept. Dieses Konzept setzt zwei Sensoren mit fast spiegelbildlich asymmetrischer Antennencharakteristik ein. So sind die Nebenkeulen für den Nahbereich nach außen gerichtet und die Hauptkeulen zeigen parallel in Fahrtrichtung. Dies hat den Nachteil, dass zweimal der gleiche Sensor benötigt wird. Jedoch können diese Kosten ausgeglichen werden, indem weniger Nahbereichssensoren eingesetzt werden müssen.

\begin{figure}[hbtp]%
\centering
\subfigure[][Mechanisches Scanning\label{fig:RadarScan}]{\includegraphics[page=1,width=0.4\textwidth,trim={6.5cm 5.7cm 8cm 4cm},clip]{pics/RadarFoV.pdf}}
    \subfigure[][Monopuls Verfahren\label{fig:RadarMonopuls}]{\includegraphics[page=2,width=0.4\textwidth,trim={6.5cm 5.7cm 8cm 4cm},clip]{pics/RadarFoV.pdf}}
\caption{Ansätze zur Winkelbestimmung beim Radar \cite{Winner.2015}}%
\end{figure}

Eine weitere Möglichkeit ist der Einsatz von Planar Antennen Arrays. Mit diesen Arrays kann ein digitaler Scan durchgeführt werden. Hierfür bestehen zwei Ansätze. Ein Ansatz nutzt einen gesteuerten Phasenschieber (ähnlich Phased Array). Ein anderer speichert den Datenstrom parallel oder seriell zu den Einzelantennen ab und wertet diesen erst in der Nachverarbeitung aus (Digital Beam Forming).

Von diesen vorgestellten Verfahren verwenden die Sensorhersteller vorwiegend Planar Antennen Arrays. Die Firma Hella hingegen setzt das Monopuls-Verfahren zur Winkelmessung ein.

\subsubsection{Lidar}
\label{sec:Lidar}

Light Detection and Ranging, kurz Lidar, gehört zu den optischen Messverfahren und nutzt \ac{UV}-, \ac{IR}-Strahlen oder Strahlen aus dem sichtbaren Spektrum \cite{Winner.2015}. Für die Abstandsmessung wird die Pulslaufzeitmessung genutzt, bei dem kurze Lichtblitze hoher Leistung, meist Laser-Pulse, gesendet werden und die Laufzeit $\Delta t$ gemessen wird \cite{Trankler.2014}. Der Abstand $D$ wird, mit der Lichtgeschwindigkeit $c_L$, analog zum Ultraschall bestimmt:
\begin{equation}
D = \dfrac{c_L \Delta t}{2}
\label{eq:AbstandL}
\end{equation}

Die Reichweite beim Lidar ist durch die Lichtintensität, welche den Laserschutzvorschriften genügen muss, beschränkt. Des Weiteren beeinflusst der Reflexionsgrad $\rho_L$ die Reichweite. Dieser ist von der Oberfläche und der Größe des Objektes und vom Einfallwinkel abhängig. Auch die Witterung beeinträchtigt die Reichweite des Lidars, da die Strahlen von Wassertropfen bei Regen und Nebel gebrochen werden.
%So ergibt sich für die reflektierte Lichtintensität $P_{r,groß}$ für ein großes bzw. nahes Objekt die folgende Gleichung:
%\begin{equation}
%P_{r,groß} = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^3 \cdot (Q_v/4)(\Phi/2)^2}
	%\label{eq:Pgross}
%\end{equation}
%
%Für ein Objekt, das -- aufgrund der Entfernung -- kleiner ist als der Lichtpunkt, ändert sich Gleichung \ref{eq:Pgross} zu:
%\begin{equation}
%P_{r,klein} = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^4 \cdot (Q_v Q_h/4)(\Phi/2)^2}. 
%\label{eq:Pklein}
%\end{equation}

%Hierbei ist $\Phi$ der Winkel der Objektreflexion, $H$ die Objektbreite, $A_t$ die Empfangslinsenfläche, $T$ die Transmission der Atmosphäre, $Q_v$ und $Q_h$ die vertikale bzw. horizontale Strahldivergenz und $P_t$ die Laserleistung.

Das Sichtfeld kann von dem eindimensionalen Fall mit einem Strahl in nur eine Richtung auch horizontal und vertikal beliebig erweitert werden. Dafür gibt es verschiedene Ansätze. Ein Ansatz ist der sogenannte mechanische Scan in Abbildung\,\ref{fig:LidarScan} bei dem ein schwenkbarer Spiegel eingesetzt wird, der den Laserstrahl umlenkt. Hierdurch ist ein horizontaler Öffnungswinkel von bis zu \unit[360]{°} und ein vertikaler Öffnungswinkel von bis zu \unit[120]{°} möglich. Eine Variante vom Scan ist der "`Sweep"', siehe Abbildung \ref{fig:LidarSweep}, bei dem ein Bündel von Laserstrahlen geschwenkt wird. Ein weiterer Ansatz nutzt ein Array aus Laserdioden, siehe Abbildung\,\ref{fig:LidarArray}, die angesteuert werden. Eine Möglichkeit der Ansteuerung ist das Multiplexverfahren, bei dem die Dioden nacheinander ansteuert und so ein digitaler Scan durchgeführt wird.  Der horizontale Öffnungswinkel hierbei beträgt bis zu \unit[110]{°}. Der vertikale Öffnungswinkel ist von der Strahlbreite abhängig und beträgt in etwa \unit[2]{°} bis \unit[5]{°}. Bei einer weiteren Variante wird das horizontale Array in der Vertikalen erweitertet und die Dioden werden gleichzeitig angesteuert, sodass ein dreidimensionales Abbild der Umwelt erzeugt wird.

\begin{figure}[hbtp]%
\centering
\subfigure[][Scanning\label{fig:LidarScan}]{\includegraphics[page=1,width=0.32\textwidth,trim={2cm 8.9cm 15.2cm 1.35cm},clip]{pics/LidarFoV.pdf}}
    \subfigure[][Sweepen\label{fig:LidarSweep}]{\includegraphics[page=2,width=0.32\textwidth,trim={2cm 8.9cm 15.2cm 1.35cm},clip]{pics/LidarFoV.pdf}}
		\subfigure[][Arrays\label{fig:LidarArray}]{\includegraphics[page=3,width=0.32\textwidth,trim={2cm 8.9cm 15.2cm 1.35cm},clip]{pics/LidarFoV.pdf}}
\caption{Ansätze zur Winkelbestimmung beim Lidar \cite{Winner.2015}}%
\end{figure}

Neben dem Einsatz von einzelnen Lidarsensoren werden vermehrt Lidarsensoren mit Kameras kombiniert. Diese Sensorkombination wird \ac{PMD} genannt. Hierbei wird jedem Bildpunkt der Kamera direkt ein Abstand mit Hilfe des Lidars zugeordnet.

Seit dem vermehrt auch die Mittel- und Kompaktklasse mit Fahrerassistenzsystemen wie z.B. \ac{ACC} ausgestattet werden, kommen zunehmend auch Lidar Sensoren zum Einsatz. Diese Entwicklung ist zum Einen der Kostenoptimierung der Sensoren und zum Anderen dem erfolgreichen Einsatz von Lidarsensoren bei der "`DARPA Urban Challenge"' zum autonomen Fahren  geschuldet \cite{Winner.2015}.  

\subsubsection{Kamera}
\label{sec:Kamera}
%Einbau hinter Windschutzscheibe am Rückspiegel
%bei FIR Einbau im Scheinwerfer oder Kühlerbereich wegen geringer Scheibentransmission
%
%Umfeldkameras:
%großer Blickwinkel
%stellt Fahrer aufgenommenes Bild zur Verfügung
%
%Rückfahr:
%Heckklappe auf Höhe des Nummernschildes
%Unfälle mit übersehenden Personen vermeiden
%Parkvorgang unterstützen
%
%Surround View:
%vier oder mehr Kameras rund um Fahrzeug
%360° Ansicht der Umgebung erzeugt
%
%Spiegelersatz:
%Ersatz von Außenspiegeln
%geringerer Kraftstoffverbrauch


Die Kamera gehört zu den bildgebenden Sensoren und hat den Vorteil, ähnliche Informationen wie das menschliche Auge zu produzieren \cite{Winner.2015}. Somit werden alle relevanten Verkehrsmerkmale aufgenommen und können gut identifiziert werden.

Bildsensoren für den Verkehr müssen je nach Einsatzbereich unterschiedliche Anforderungen erfüllen. Für die Unterscheidung von Rücklichtern und Scheinwerfern und auch anderen Bildelementen sind farbsensitive Systeme erforderlich. Durch die extremen Lichtunterschiede bei Nacht ist außerdem ein hoher Dynamikbereich gefordert. Für die Verkehrszeichenerkennung wird eine hohe Auflösung ($>$ \unit[15]{Pixel/°}) benötigt. Bei Autobahnfahrten ist neben einer kurzen Belichtungszeit ($<$ \unit[30]{ms}) ebenfalls diese hohe Auflösung gefordert. Für die Fußgängerdetektion ist hingegen ein breites Sichtfeld wichtig \cite{Winner.2015}.

Der Nachteil beim Einsatz einer Kamera ist die indirekte Abstandsmessung. Bei einer Monokamera kann der Abstand nur anhand der Auflösung geschätzt werden. Aus diesem Grund wird die Kamera häufig mit anderen Sensoren kombiniert wie beispielsweise mit einem Lidar (siehe Abschnitt\,\ref{sec:Lidar}). Eine weitere Möglichkeit ist der Einsatz von zwei Kameras. Diese Stereokonfiguration erfordert neben einer weiteren Kamera noch zusätzliche Recheneinheiten und eine höhere Rechenleistung. Bei dieser Konfiguration muss außerdem beachtet werden, dass die Module synchronisiert und zueinander in allen Raumachsen stabil montiert sind. Nur so kann eine zeitliche und räumliche Verschiebung der Bilder und eine Dekalibrierung ausgeschlossen werden. Eine rektifizierte Darstellung, d.h. mit eliminierten geometrischen Verzerrungen, der zueinander verdrehten Stereobildebenen ist in Abbildung\,\ref{fig:Stereo} dargestellt.

\begin{figure}[hbtp]%
\centering
\includegraphics[width=0.8\columnwidth,trim={1.5cm 6.5cm 5.5cm 3.5cm},clip]{pics/Stereoaufbau.pdf}%
\caption{Rektifiziertes Stereokamerasystem}%
\label{fig:Stereo}%
\end{figure}

Der Abstand $z_C$ zum Punkt $X_W$ in der Realität berechnet sich bei einem Stereosystem mit der Basisbreite $b$ (Abstand der Kameras), der Brennweite $f$, der Disparität $d$ (Abstand der Bildpunkte) und der Größe des Bildpunktes $s_p$ auf dem Sensor folgendermaßen:
\begin{equation}
z_C = \dfrac{b \cdot f}{d \cdot s_p}
\label{eq:dStereo}
\end{equation} 

Neben dem sichtbaren Spektrum wird auch das Infrarotspektrum eingesetzt. Dies ist der unzureichenden Empfindlichkeit bei anspruchsvollen Lichtverhältnissen geschuldet. Hier kommen zwei verschiedene Ansätze zum Einsatz. Dies ist einmal das Ausleuchten der Szene mittels spezieller Scheinwerfer mit infrarotem Licht, das vom Kamerasystem mit Hilfe eines Filters aufgenommen wird. Der zweite Ansatz nutzt Spezialkameras, die für die Wärmestrahlung (Ferninfrarot) von Körpern empfindlich sind. Diese Systeme sind kostenintensiv, da ein spezieller Bildsensor eingesetzt werden muss.

%Die Entfernungsmessung mit einer einzelnen Kamera ist hingegen ungenau, da nur anhand der Auflösung geschätzt werden kann. Auch der Sichtbereich und die Reichweite sind von der Auflösung abhängig. Letzteres wird durch den Bereich des scharfen Abbildens begrenzt \cite{Hering.2016}. Die untere Grenze $a_v$ der Reichweite liegt vor und die obere Grenze $a_h$ hinter der Objektebene. Sie ergeben sich mit
%\begin{align}
%a_v &= \dfrac{a f'^2}{f'^2 - u'k(a+f')}\\
%a_h &= \dfrac{a f'^2}{f'^2 + u'k(a+f')}.
%\label{eq:a_vh}
%\end{align}
%
%Dabei ist $k$ die Blendenzahl, $a$ der Abstand zwischen Objektebene und Eintrittspupille, $f'$ die Brennweite und $u'$ der Durchmesser des Unschärfekreises, der sich folgendermaßen bestimmen lässt:
%\begin{equation}
%u' = \dfrac{\text{Formatdiagonale}}{1000}
%\label{eq:u'}
%\end{equation}
%
%Neben dem sichtbaren Spektrum können einige Kameras auch das Infrarotspektrum abbilden. Dadurch ist der Einsatz in der Nacht bzw. Dunkelheit möglich. Es gibt zwei verschiedene Ansätze hierbei, die unterschiedliche Infrarotbereiche nutzen \cite{Winner.2015}. Eine Möglichkeit ist das Nahinfrarot (NIR). Hierbei wird die Szene mit NIR-Strahlung ausgeleuchtet, dessen Reflexion von der Kamera erkannt wird. Die andere Möglichkeit ist der Einsatz von Ferninfrarot (FIR). In diesem Spektrum liegt die Wärmestrahlung von Objekten, die von speziellen Wärmebildkameras erfasst werden kann. Ein aktives Beleuchten der Szene enfällt somit.



%\subsubsection{Sensorvergleich}
%\label{sec:Sensorvergleich}
%
%
%\begin{table}[hb]%
%\centering
%\begin{tabularx}{\textwidth}{lp{2.5cm}p{2.5cm}p{2cm}p{2cm}}
 %& \textbf{Ultraschall} & \textbf{Radar} & \textbf{Lidar} & \textbf{Kamera}\\ \toprule
%\textbf{hor. Öffnungswinkel} & \unit{120}{°} -- \unit{140}{°}& \unit{12}{°} -- \unit{150}{°} & \unit{10}{°}--\unit{360}{°} & \unit{40}{°}--\unit{180}{°}\\ \midrule
%\textbf{ver. Öffnungswinkel} &\unit{60}{°} -- \unit{70}{°}& \unit{4}{°} -- \unit{36}{°}&\unit{2}{°} -- \unit{70}{°}&\unit{6.7}{°} -- \unit{54}{°}\\ \midrule
%\textbf{min. Reichweite} &\unit{0.15}{m}&\unit{0.5}{m}&\unit{0}{m}&--\\ \midrule
%\textbf{max. Reichweite} &\unit{5.5}{m}&\unit{350}{m}&\unit{300}{m}&\unit{3390}{m}\\ \midrule
%\textbf{Geschwindigkeit} & -- & \unitfrac[--400]{km}{h}\newline \unitfrac[+200]{km}{h} & -- & -- \\\midrule
%\textbf{Auflösung} & gering & \unit{0.5}{m} -- \unit{1}{m} \newline \unit{3.3}{°} -- \unit{6.}{°} \newline \unitfrac[0.1]{m}{s} -- \unitfrac[1.4]{m}{s}&\unit{0.1}{m}\newline \unit{0.25}{°}& \unit{320x240}{Pixel}\newline \unit{6576x4384}{Pixel}\\ \midrule
%\textbf{Genauigkeit} & -- &\unit{$\pm$0.25}{m}&\unit{$\pm$0.04}{m}& \unit{5.5x5.5}{$\mu$m}\\ \midrule
%\textbf{Messdauer} & \unit{1}{ms}&\unit{50}{s}& \unit{40}{Hz}& \unit{30}{Hz}\\ \bottomrule
%\end{tabularx}
%\caption{Vergleich der Parameter der Sensortypen}
%\label{tab:Sensorparameter}
%\end{table}

\subsection{Datenverarbeitung}
\label{sec:Datenverarbeitung}

Im folgenden Abschnitt wird auf die Datenverarbeitung genauer eingegangen. Sie unterteilt sich in die Objekterkennung und das Objekttracking, welche durch eine Datenfusion erweitert werden können. Der Ablauf der Datenverarbeitung ist in Abbildung\,\ref{fig:Datenverarbeitung} schematisch dargestellt.

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=\textwidth,trim={3cm 8cm 3cm 7cm},clip]{pics/Datenverarbeitung.pdf}
	\caption{Schematischer Ablauf der Datenverarbeitung \cite{Winner.2015}}
	\label{fig:Datenverarbeitung}
\end{figure}

Als erstes wird die Signalaufnahme mit dem Sensor durchgeführt. Dem schließt sich sich eine Signalverarbeitung der Rohsignale an. Danach werden mit Hilfe einer Merkmalsextraktion Merkmals- bzw. Objekthypothesen aufgestellt. Fehlerquellen sind hierbei Artefakte durch Verletzung von physikalischen Annahmen in der Signalverarbeitung und Fehlinterpretationen durch Annahmen in der Merkmalsextraktion.

\subsubsection{Objekterkennung}
\label{sec:Objekterkennung}
Die Objekterkennung stellt mit Hilfe von Merkmalen Objekthypothesen auf. Die Wahl des Verfahrens ist hierbei maßgeblich abhängig vom Sensortyp. So werden für Radar und Lidar z.B. Segmentierungsverfahren eingesetzt und für Kamera z.B. Gradientenverfahren.

Radar und Lidar erzeugen bei der Messung Punktwolken. Um in diesen Punktwolken Objekte zu identifizieren, werden Segmentierungsverfahren eingesetzt. Die Annahme bei diesem Verfahren ist, dass Messrohpunkte eines Objektes in enger Nachbarschaft liegen. So werden diese Punkte mittels Region-Growing oder Linienextraktion gruppiert bzw. verbunden, siehe Abbildung\,\ref{fig:RegionGrowingLinie}. Nach diesem Schritt werden die Segmente in I- und L-Formen unterschieden. Unter Verwendung der Segmentmaße kann schließlich die Objektklasse bestimmt werden \cite{Walchshausl.2008}.

\begin{figure}[hbtp]%
\centering
\includegraphics[width=0.6\textwidth]{pics/RegionGrowingVSLinie.PNG}%
\caption[Vergleich von zwei Bilderkennungsalgorithmen]{Beispiel für den Vergleich von Region-Growing (grün) und Linienextraktion (blau). Die roten Punkte sind die Laserscannerrohdaten \cite{Walchshausl.2008}}%
\label{fig:RegionGrowingLinie}%
\end{figure}

Bei der Kamera wird in Einzelbildmerkmale und Korrespondenzmerkmale unterschieden \cite{Winner.2015}. Erstere werden aus dem Grauwertmuster eines Bildes ermittelt und letztere nutzen die Bildpunkte der Projektion eines Punktes in mehreren Bildern. Um diese zu extrahieren, werden beispielsweise Gradienten- oder Matching Verfahren eingesetzt. Für die Extraktion von Einzelbildmerkmalen wird die Tatsache genutzt, dass sich das Bildsignal an Kanten und Ecken stark ändert. Diese Änderung kann mathematisch mit Hilfe von Gradienten beschrieben werden. Anschließend können Linienstrukturen aus lokalen Gradientenmaxima mittels Kantendetektoren extrahiert werden (Abbildung\,\ref{fig:Merkmalsextraktion}).

\begin{figure}[hbtp]%
\centering
\includegraphics[width=0.9\textwidth]{pics/Merkmalsextraktion_Kamera.PNG}%
\caption[Vergleich zwischen Kanten- und Eckendetektor.]{Vergleich zwischen Kanten- und Eckendetektor. Links: Originalbild; Rechts: Ergebnisbild eines (a) Kantendetektors; (b) Eckendetektors \cite{Winner.2015}}%
\label{fig:Merkmalsextraktion}%
\end{figure}

Korrespondenzmerkmale werden durch die Projektion eines realen Punktes in mehreren Bildern einer Bildfolge ermittelt. Dies wird entweder mit Gradientenverfahren oder mit Matching Verfahren durchgeführt. Bei ersterem wird die Intensität der Grauwerte als orts-zeitabhängige Funktion mit einer Reihenentwicklung approximiert. Beim Matching Verfahren wird eine kleine Region um einen Bildpunkt herum mit den entsprechenden Punkten im nächsten Bild verglichen, um Korrespondenzen zu finden. Um nicht den gesamten Bildraum abzusuchen, wird das Verfahren auf markante Bildstrukturen wie Ecken und Kanten im Bild beschränkt.

%Bei der Kamera werden Gradientenverfahren und Matching Verfahren zur Merkmalsextraktion eingesetzt \cite{Winner.2015}. Gradientenverfahren approximieren mit einer Reihenentwicklung zunächst die Intensität Grauwerte als orts-zeitabhängige Funktion $g(x,y,t)$. Die wichtigsten Merkmale beim Kamerabild sind Kanten und Ecken. Diese führen zu einer deutlichen Änderung des Bildsignals, welche mathematisch durch Gradienten beschrieben wird. Diese können in Histogramme der Gradientenrichtung überführt werden, siehe Abbildung \ref{fig:Bildgradienten}. 

\subsubsection{Tracking}
\label{sec:Tracking}
Der Objekterkennung schließt sich das Objekttracking an, bei dem die erkannten Objekte verfolgt werden. Das bedeutet, dass die neuen Objekthypothesen mit den vorherigen verglichen und ggf. zugeordnet werden.

Für das Tracking werden insbesondere drei verschiedene Verfahren eingesetzt. Das sind der Bayes-Filter, der Kalman-Filter und der Partikelfilter \cite{Winner.2015}. Die Aufgabe dieser Verfolgungsverfahren ist, aus den Beobachtungen $Y_k$ die zu schätzenden Größen $X_k$ zu bestimmen. Dies geschieht zu diskreten Zeitschritten $k=1,\, 2,...$. Die Systemgleichung 
\begin{equation}
X_k = f_k\left(X_{k-1},s_k\right)
\label{eq:ZustandXk}
\end{equation}

beschreibt die Dynamik des Zustandes $X_k$. Hierbei wird das stochastische Systemrauschen $S$ mit Hilfe von $s_k$ realisiert. Die erzeugten Beobachtungen $Y_k$ werden mittels der Beobachtungsgleichung
\begin{equation}
Y_k = g_k\left(X_k,v_k\right)
\label{eq:BeobachtungYk}
\end{equation}

beschrieben. $v_k$ ist dabei die Realisierung des stochastischen Beobachtungsrauschens $V$. Mit Hilfe dieser Gleichungen wird die Wahrscheinlichkeitsdichte $p\left(X_k|Y_0...Y_k\right)$ für den aktuellen Zustand geschätzt.

Der Bayes-Filter ist ein allgemeingültiges Verfolgungsverfahren. Es schätzt aus der Beobachtung die neue mögliche Position. Die Beobachtung findet im Zustandsraum statt und gibt eine Wahrscheinlichkeitsdichte für den aktuellen Zustand heraus unter Berücksichtigung aller vorigen Beobachtungen. 
%Die rekursive Gleichung für die Wahrscheinlichkeitsdichte des Bayes-Filters lautet folgendermaßen:
%\begin{equation}
%p\left(X_k|Y_0,...,Y_{k-1}\right)= c \cdot p\left(Y_k|X_k\right)\cdot \int p\left(X_k|X_{k-1}\right)p\left(X_{k-1}|Y_0,...,Y_{k-1}\right) dX_{k-1}
%\label{eq:Bayes}
%\end{equation}

Der Kalman-Filter schätzt die Zustände aufgrund von redundanten Daten. Somit wird zu jedem Zeitpunkt $k$ die Normalverteilung mit Hilfe ihres Mittelwertes $\hat{X_k}$ und der Kovarianzmatrix $P_k$ bestimmt. Die  Schätzung aus dem vorigen Schritt $\hat{X}_{k-1}$, $P_{k-1}$ wird für die nächste Position auf den aktuellen Zeitschritt projiziert.
%\begin{align}
%\hat{X}^{-}_{k}&=F\hat{X}_{k-1}\\
%\hat{P}^{-}_{k}&=F\hat{P}_{k-1}F^T+P_S
%\end{align}
%
%Dabei ist $F$ die Dynamikmatrix und $P_S$ die Kovarianzmatrix des Systemrauschens. 
Danach wird schließlich die neueste Beobachtung $Y_k$ mit der Beobachtungsmatrix $G$ und der Kovarianzmatrix des Beobachtungsrauschens $P_V$ berücksichtigt.
%\begin{align}
%\hat{X}_{k}&=\hat{X}^{-}_{k} + \hat{P}^{-}_{k}G^T\left(P_V + G\hat{P}^{-}_{k}G^T\right)^{-1} \left(Y_k - GF\hat{X}_{k-1}\right)\\
%\hat{P}_{k}&=\hat{P}^{-}_{k}-\hat{P}^{-}_{k}G^T \left(P_V + G\hat{P}^{-}_{k}G^T\right)^{-1}G\hat{P}^{-}_{k}
%\end{align}

Beim Partikelfilter wird die Wahscheinlichkeitsdichte durch die endliche Summe von Diracstößen mit Gewichten $w^{i}_{k}p\left(X_k|Y_0,...,Y_k\right)\approx \sum w^{i}_{k}\delta\left(X_k - X^{i}_{k}\right)$ approximiert. Die Paare aus Gewicht $W^{i}_{k}$ und Zustand $X^{i}_{k}$ werden als Partikel betrachtet. Nach jedem Innovationsschritt werden schließlich die Gewichte aktualisiert.

\subsubsection{Sensordatenfusion}
\label{sec:Fusion}

Die Sensordatenfusion wird genutzt, um die Genauigkeit zu erhöhen bzw. mehr Informationen zu erhalten. Dies ist davon abhängig, welche Sensoren und wie sie eingesetzt werden. Die Sensoren können hierbei komplementär, konkurrierend oder kooperativ verbaut sein. Diese verschiedenen Verbauweisen sind in Abbildung\,\ref{fig:Fusionsansaetze} dargestellt. Werden Sensoren komplementär genutzt, so ergänzen sich deren Sichtfelder zu einem großen. Sind Sensoren konkurrierend verbaut, sind sie entweder redundant, das heißt es wird die gleiche Information generiert, oder konträr, also werden gegensätzliche Informationen erzeugt. Die letzte Variante ist der kooperative Einsatz von unterschiedlichen Sensoren, die zusammen einen höheren Informationsgehalt erzeugen \cite{Dietmayer2005}.

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.6\textwidth,trim={3cm 6.5cm 9cm 4cm},clip]{pics/Fusionsansaetze.pdf}
	\caption{Ansätze der Datenaufnahme für die Datenfusion \cite{Dietmayer2005}}
	\label{fig:Fusionsansaetze}
\end{figure}

Für die Sensordatenfusion existieren zwei Ansätze. Entweder werden die Daten implizit oder explizit fusioniert. Bei der expliziten Fusion (Abbildung\,\ref{fig:ExpliziteFusion}) wird abgewartet, bis alle Messdaten vorliegen und dann erst fusioniert. Somit findet eine zeitliche Filterung in einem festen Zeitraster statt und die Assoziation findet auf einem gemeinsamen Abstraktionslevel statt. Hierbei müssen die Messdaten jedoch synchronisierbar sein.

\begin{figure}[hbtp]
	\centering
		\includegraphics[width=0.9\textwidth]{pics/ExpliziteFusion.PNG}
	\caption{Explizite Sensordatenfusion mit synchronen Sensoren \cite{Dietmayer2005}}
	\label{fig:ExpliziteFusion}
\end{figure}

Die Datenfusion mit synchronen Sensoren führt zu einer sicheren und zuverlässigen Assoziation. Da der Sensor mit der längsten Akquisitionszeit den zeitlichen Versatz zwischen Messung und Assoziation bestimmt, entsteht ein hoher Zeitversatz zwischen Realwelt und Modell $\Delta T_{Delay_{Min}}$. Die Arbeit mit synchronisierten Sensoren bietet neben der sicheren Assoziation eine einfache Erweiterbarkeit um weitere Sensoren, da der Algorithmus deterministisch ist.

Im Gegensatz zur expliziten Fusion werden die Sensordaten bei der impliziten Fusion zeitlich nacheinander eingebracht. Dadurch wird eine zeitlich konsistente Datenverarbeitung nötig. Außerdem muss eine zeitliche Filterung durchgeführt werden, wenn die Messdaten vorliegen. Außerdem wird in der Regel schon eine Assoziation auf dem sensorspezifischen Abstraktionslevel durchgeführt, die bei der Fusion abgeglichen wird. Bei der impliziten Fusion muss keine Synchronisierung der Sensoren durchgeführt werden.

Werden asynchrone Sensoren bei der Datenfusion verwendet, müssen die Daten sequentiell eingebracht werden. Dies bedeutet, dass der Algorithmus nicht deterministisch sein kann, da er dynamisch auf die neuen Daten reagieren muss. Bei diesem Verfahren können Quantisierungsfehler auftreten. Insbesondere wenn die Daten des Sensors mit der kleinsten Latenz kurz nach denen vom Sensor mit der größten Latenz eingebracht werden. Dann können diese Informationen nicht mehr in der Assoziation mitberücksichtigt werden. Je ähnlicher die Latenzzeiten der einzelnen Sensoren sind, desto geringer wird schließlich der Fehler der Schätzung. 

%\begin{figure}[h]
	%\centering
		%\includegraphics[width=0.9\textwidth]{pics/ImpliziteFusion.PNG}
	%\caption{Implizite Sensordatenfusion mit asynchronen Sensoren \cite{Dietmayer2005}}
	%\label{fig:ImpliziteFusion}
%\end{figure}
%



	
%\end{table}
