\section{Umfelderfassung}
\label{sec:TechnikStand}
Dieses Kapitel stellt zunächst die bestehenden Sensoren zur Umfelderfassung vor und erläutert deren Funktionsprinzipien. Anschließend wird auf die Sensordatenverarbeitung eingangen. Dazu gehören die Datenfusion, die Objekterkennung und das Tracking. Auf diesen Grundlagen aufbauend wird in den Abschnitten \ref{sec:KFZSensor} und \ref{sec:InfraSensor} aufgezeigt, wie dies im Fahrzeug und auch in der Infrastruktur genutzt wird.

\subsection{Sensoren zur Umfelderfassung}
\label{sec:Sensoren}
Die Sensoren zur Umfelderfassung werden in entfernungsgebende und bildgebende Sensoren unterschieden. Zu ersterem gehören der Ultraschall, der Radar und der Lidar. Zu letzterem die Kamera mit dem sichtbaren und dem Infrarotspektrum. Im Folgenden werden die einzelnen Sensorprinzipien erläutert.

\subsubsection{Ultraschall}
\label{sec:Ultraschall}

Als Ultraschall werden die Schallfrequenzen ab \unit{20}{kHz} bezeichnet. Sie gehören zu den Frequenzen, die für das menschliche Ohr nicht hörbar sind. Die Messung mit Ultraschall gehört zu den Laufzeitmessungen. Ein Sender emittiert Schallwellen, die schließlich von Objekten reflektiert werden. Mit Hilfe der gemessenen Laufzeit $\Delta t$, bis das Echo wieder am Sender ankommt, kann der Abstand $d$ zum gemessenen Objekt mit Hilfe der Schallgeschwindigkeit $c_S$ bestimmt werden \cite{Trankler.2014}:

\begin{equation}
d = \dfrac{c_S \Delta t}{2}.
\label{eq:AbstandU}
\end{equation}

Da die Strecke zwischen Sender und Objekt zweimal durchlaufen wird, muss diese halbiert werden um den tatsächlichen Abstand zu erhalten. Neben der reinen Abstandsmessung kann mit Hilfe von zwei Sendern, deren Erfassungsbereiche überlappen, auch eine Positionsbestimmung durchgeführt werden. Hierzu wird das Trilaterationsverfahren genutzt. Für ein rundes Objekt ist dies in Abbildung \ref{fig:Trilateration} dargestellt.\marginpar{Abbildung überarbeiten!} Der Abstand $D$ berechnet sich mittels des Satzes von Pythagoras:

\begin{equation}
D = \sqrt{DE1^2 - \dfrac{\left(d^2 + DE1^2-DE2^2\right)^2}{4d^2}}
\label{eq:PosD}
\end{equation}

\begin{figure}%
\centering
\includegraphics[width=0.6\textwidth,trim={2cm 11cm 16cm 3.5cm},clip]{pics/Position_Ultraschall.pdf}%
\caption{Veranschaulichung des Trilaterationsprinzips für ein rundes Objekt}%
\label{fig:Trilateration}%
\end{figure}

Die Reichweite des Ultraschallsensors ist abhängig von der ausgesendeten Schallintensität $I_S$, die in Abhängigkeit von der Entfernung $r$ des gemessenen Objektes abnimmt. Somit ergibt sich mit der effektiven Reflexionsfläche $\sigma$ und bezogen auf den Normabstand $r_1$ die reflektierte Schallintensität 

\begin{equation}
I_{refl}=\sigma I_s \left(\dfrac{r_1}{2r}\right)^2.
\label{eq:Irefl}
\end{equation}

Des Weiteren verringern der Reflexionsgrad $\rho_S$ und die Impedanz der Atmosphäre die Schallintensität bei der Reflexion. Als untere Grenze zur Objektmessung muss die Schallintensität des Empfangssignals oberhalb des Rauschens liegen, d.h. \unit{$\geq$10}{dB} sein.

Zur Schallerzeugung und -empfang wird bei Ultraschallsensoren eine Membran aus einer Piezokeramik eingesetzt \cite{Winner.2015}. Zum Aussenden der Schallwellen wird die Membran aktiv in Schwingung versetzt und nach einer festgelegten Sendedauer wieder zur Ruhe gebracht. Die Zeit bis der reflektierte Schall die Membran wieder zur Schwingung anregt wird mit Gleichung \ref{eq:AbstandU} zur Abstandsbestimmung genutzt.

\subsubsection{Radar}
\label{sec:Radar}

Die Radarmessung (Radio Detection And Ranging) gehört zu den berührungslosen Messverfahren und wird insbesondere bei anspruchsvollen Umgebungsbedingungen eingesetzt \cite{Trankler.2014}. Bei diesem Verfahren werden elektromagnetische Wellen im Mikrowellenbereich eingesetzt, welche kaum anfällig gegenüber Temperaturschwankungen und Nebel sind. Für den Automobilbereich sind die Frequenzen \unit{24}{GHz} und \unit{77}{GHz} reserviert \cite{Winner.2015}.

Zur Abstandsmessung finden zwei verschiedene Ansätze Verwendung, die sich in der Frequenzmodulation unterscheiden. Das ist einmal das Dauerstrichradar (FMCW -- Frequency Modulated Continous Wave) und zum anderen die Chirp Frequence Modulation. Die Frequenzverläufe der beiden Modulationsverfahren sind in Abbildung \marginpar{Abbildung einfügen!} dargestellt. Das Dauerstrichradar erzeugt durch die kontinuierliche und rampenförmige Veränderung der Momentanfrequenz eine konstante Phasenverschiebung von $+(2r/c)^2m_{\omega}$. Der Abstand und die Geschwindigkeit wird bei diesem Verfahren mithilfe der Frequenzverschiebung bestimmt:

\begin{align}
	r &= \dfrac{c}{2} \cdot \dfrac{\omega_{obj,1}-\omega_{obj,2}}{m_{\omega,1} - m_{\omega,2}},\\
	\dot{r} &= \dfrac{c}{2 \omega_0} \cdot \dfrac{m_{\omega,1}\omega_{obj,1}-m_{\omega,2}\omega_{obj,2}}{m_{\omega,1} - m_{\omega,2}}.
	\end{align}

Bei der Chirp Frequence Modulation wird ein Sägezahnsignal mit einem Hub von $f_{chirp}=30...300\,\text{MHz}$ erzeugt. Die Dopplerfrequenz bestimmt hierbei die Wiederholrate und sollte etwa \unit{80}{kHz} betragen, um Mehrdeutigkeiten zu vermeiden. Der Abstand wird mit dem Puls-Doppler-Verfahren mit der Laufzeit $t_{of}=t_{PC}-t_S$, bezogen auf die Pulsmitte $t_{PC}$, bestimmt:

\begin{equation}
	r = \dfrac{1}{2}c t_{of}
\label{eq:AbstandRadar}
\end{equation}

Für die Bestimmung der Geschwindigkeit wird beim Radar der Doppler-Effekt genutzt. Der Doppler-Effekt besagt, dass sich die Frequenz bei der Reflexion in Abhängigkeit von der Änderung des Abstandes $\dot{r}$ ändert. Diese Frequenz wird Dopplerfrequenz $f_{Doppler}$ genannt und ergibt sich mit der Trägerfrequenz $f_0$ und der Lichtgeschwindigkeit $c$ folgendermaßen:

\begin{equation}
f_{Doppler} = -\dfrac{2 \dot{r}}{c} f_0
\label{eq:Doppler}
\end{equation}

Bei einer Annäherung ($\dot{r}<0$) ist diese Frequenz positiv und beim Entfernen negativ.

Die Reichweite des Radars ist abhängig von der Sendeleistung und der Richtcharakteristik der Antenne. Je nach Richtcharakteristik ergibt sich der Antennengewinn $G$, der Einfluss auf die Reichweite nimmt. So ergibt sich die Empfangsleistung für ein reflektiertes Radarsignal zu

\begin{equation}
P_R = \dfrac{10^{-2kr/1000}  \sigma  \lambda^2  G^2  V_{mp}^2  P_{total}}{(4\pi)^3 r^4}
\label{eq:Empfangsleistung}
\end{equation}

mit dem Rückstreuquerschnitt des Objektes

\begin{equation}
\sigma_{plate} = 4\pi \dfrac{A^2}{\lambda^2}.
\label{eq:Rueckstreuquerschnitt}
\end{equation}

Gleichung \ref{eq:Empfangsleistung} berücksichtigt außerdem sogenannte Signalleistungsschüttler mit dem Faktor $V_{mp}^2$, $0 \leq V_{mp} \leq 2$.

Für die Winkelbestimmung kommen zwei verschiedene Verfahren zum Einsatz. Das ist zum Einen das mechanische Scanning, bei dem eine Strahlablenkeinheit oder eine Planarantenne mechanisch geschwenkt wird. Dabei bewegt sich die Radarkeule mit einer Schrittweite von etwa \unit{1}{°}, siehe Abbildung \marginpar{Abbildung einfügen}. Das zweite Verfahren ist das Monopuls-Verfahren. Hierbei erzeugt eine separate Antenne einen Sendestrahl, der von einer Doppelantennen-Anordnung Empfangen wird, siehe Abbildung \marginpar{Abbildung einfügen}. Eine Verbesserung dieses Verfahrens ermöglicht die Verwendung von Mehrstrahlern mit bis zu vier Antennen.

\subsubsection{Lidar}
\label{sec:Lidar}

Light Detection and Ranging, kurz Lidar, gehört zu den optischen Messverfahren und nutzt UV-, IR-Strahlen oder Strahlen aus dem sichtbaren Spektrum \cite{Winner.2015}. Für die Abstandsmessung wird die Pulslaufzeitmessung genutzt bei dem kurze Lichtblitze hoher Leistung, meist Laser-Pulse, gesendet und die Laufzeit gemessen wird \cite{Trankler.2014}. Der Abstand wird ähnlich wie beim Ultraschall bestimmt:

\begin{equation}
d = \dfrac{c_L \Delta t}{2}
\label{eq:AbstandL}
\end{equation}

Mit der Lichtgeschwindigkeit $c_L$. Die Reichweite beim Lidar ist durch die Lichtintensität, welche den Laserschutzvorschriften genügen muss, beschränkt. Des Weiteren beeinflusst auch der Reflexionsgrad $\rho_L$ die Reichweite. Dieser ist insbesondere von der Oberfläche des Objektes abhängig, aber auch von seiner Größe. So ergibt sich für die reflektierte Lichtintensität $P_r$ für ein großes bzw. nahes Objekt die folgende Gleichung:

\begin{equation}
P_r = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^3 \cdot (Q_v/4)(\Phi/2)^2}
	\label{eq:Pgross}
\end{equation}

Für ein Objekt, das -- aufgrund der Entfernung -- kleiner ist als der Lichtpunkt, ändert sich Gleichung \ref{eq:Pgross} zu:

\begin{equation}
P_r = \dfrac{\rho_L \cdot A_t \cdot H \cdot T^2 \cdot P_t}{\pi^2 \cdot R^4 \cdot (Q_v Q_h/4)(\Phi/2)^2}. 
\label{eq:Pklein}
\end{equation}

Hierbei ist $\Phi$ der Winkel der Objektreflexion, $H$ die Objektbreite, $A_t$ die Empfangslinsenfläche, $T$ die Transmission der Atmosphäre, $Q_v$ und $Q_h$ die vertikale bzw. horizontale Strahldivergenz und $P_t$ die Laserleistung.

Das Sichtfeld kann von dem eindimensionalen Fall mit einem Strahl in nur eine Richtung auch horizontal und vertikal beliebig erweitert werden. Dafür gibt es zwei verschiedene Ansätze. Ein Ansatz ist der Einsatz eines schwenkbaren Spiegels, der den Laserstrahl umlenkt. Hierdurch ist ein horizontaler Öffnungswinkel von bis zu \unit{360}{°} und ein vertikaler Öffnungswinkel von bis zu \unit{120}{°} möglich. Der zweite Ansatz nutzt ein Array aus Laserdioden, die mittels Multiplexverfahren angesteuert werden. Der horizontale Öffnungswinkel bei dieser Variante beträgt bis zu \unit{110}{°} und der vertikale Öffnungswinkel ist von der Strahlbreite abhängig und beträgt in etwa \unit{2}{°} bis \unit{5}{°}.

\subsubsection{Kamera}
\label{sec:Kamera}
Die Kamera gehört zu den bildgebenden Sensoren und besitzt dadurch den Vorteil, ähnliche Informationen wie das menschliche Auge zu produzieren. Somit können Objekte mit einer hohen Genauigkeit identifiziert werden. Jedoch ist die Entfernungsmessung mit einer Monokamera eher ungenau, da dies nur anhand der Auflösung geschätzt werden kann. Diese bestimmt auch den Sichtbereich und die Reichweite. Letzteres wird durch den Bereich des scharfen Abbildens begrenzt \cite{Hering.2016}. Die untere Grenze $a_v$ der Reichweite liegt vor und die obere Grenze $a_h$ hinter der Objektebene. Sie ergeben sich mit

\begin{align}
a_v &= \dfrac{a f'^2}{f'^2 - u'k(a+f')}\\
a_h &= \dfrac{a f'^2}{f'^2 + u'k(a+f')}.
\label{eq:a_vh}
\end{align}

Dabei ist $k$ die Blendenzahl, $a$ der Abstand zwischen Objektebene und Eintrittspupille, $f'$ die Brennweite und $u'$ der Durchmesser des Unschärfekreises, der sich folgendermaßen bestimmen lässt:

\begin{equation}
u' = \dfrac{\text{Formatdiagonale}}{1000}
\label{eq:u'}
\end{equation}

Neben dem sichtbaren Spektrum können einige Kameras auch das Infrarotspektrum erkennen. So kann auch bei Nacht bzw. Dunkelheit die Kamera weiterhin eingesetzt werden. Es gibt zwei verschiedene Ansätze hierbei, die unterschiedliche Infrarotbereiche nutzen \cite{Winner.2015}. Eine Möglichkeit ist das Nahinfrarot (NIR). Hierbei wird die Szene mit NIR-Strahlung ausgeleuchtet, die von der Kamera erkannt wird. Die andere Möglichkeit ist der Einsatz von Ferninfrarot (FIR). In diesem Spektrum liegt die Wärmestrahlung von Objekten, die von speziellen Wärmebildkameras erfasst werden kann.

\subsection{Datenverarbeitung}
\label{sec:Datenverarbeitung}

Im folgenden Abschnitt wird auf die Datenverarbeitung genauer eingegangen. Hierzu gehört die Objekterkennung und das Objekttracking, welche durch eine Datenfusion erweitert werden können. Der Ablauf der Datenverarbeitung ist in Abbildung \ref{fig:Datenverarbeitung} schematisch dargestellt.

\begin{figure}[h]
	\centering
		\includegraphics[width=\textwidth,trim={3cm 8cm 3cm 7cm},clip]{pics/Datenverarbeitung.pdf}
	\caption{Schematischer Ablauf der Datenverarbeitung \cite{Winner.2015}}
	\label{fig:Datenverarbeitung}
\end{figure}

Als erstes wird die Signalaufnahme mittels des Sensors durchgeführt. Dem schließt sich sich eine Signalverarbeitung der Rohsignale an. Danach werden mit Hilfe einer Merkmalsextraktion Merkmals- bzw. Objekthypothesen aufgestellt. Fehlerquellen sind hierbei Artefakte durch Verletzung von physikalischen Annahmen in der Signalverarbeitung und Fehlinterpretationen durch Annahmen in der Merkmalsextraktion.

\subsubsection{Objekterkennung und Tracking}
\label{sec:Objekterkennung}
Für die Objekterkennung und das Tracking gibt es unterschiedliche Verfahren. Bei der Objekterkennung ist die Wahl des Verfahrens insbesondere abhängig vom Sensortyp. \marginpar{Beispielbilder für Segmentierungsverfahren und Gradientenverfahren?}

Radar und Lidar erzeugen bei der Messung Punktwolken. Um in diesen Punktwolken Objekte zu identifizieren, werden Segmentierungsverfahren eingesetzt. Die Annahme bei diesem Verfahren ist, dass Messrohpunkte eines Objektes in enger Nachbarschaft liegen. So werden diese Punkte mittels Region-Growing oder Linienextraktion gruppiert bzw. verbunden. Nach diesem Schritt werden die Segmente in I- und L-Formen unterschieden. Unter Verwendung der Segmentabmessungen kann schließlich die Objektklasse bestimmt werden \cite{Walchshausl.2008}.

Bei der Kamera werden Gradientenverfahren und Matching Verfahren zur Merkmalsextraktion eingesetzt \cite{Winner.2015}. Die wichtigsten Merkmale beim Kamerabild sind Kanten und Ecken. Diese führen zu einer deutlichen Änderung des Bildsignals, welche mathematisch durch Gradienten beschrieben wird. Beim Matching Verfahren wird eine kleine Region um einen Bildpunkt herum mit den entsprechenden Punkten im nächsten Bild verglichen. Um nicht den gesamten Bildraum abzusuchen, wird das Verfahren auf Ecken und Kanten im Bild angewendet.

Für das Tracking werden insbesondere drei verschiedene Verfahren eingesetzt. Das sind der Bayes-Filter, der Kalman-Filter und der Partikelfilter \cite{Winner.2015}. Der Bayes-Filter ist ein allgemeingültiges Verfolgungsverfahren. Es schätzt aus der Beobachtung die neue mögliche Position. Die Beobachtung findet im Zustandsraum statt und gibt eine Wahrscheinlichkeitsdichte für den aktuellen Zustand heraus unter Berücksichtigung aller vorigen Beobachtungen. \marginpar{Gleichungen?}

Der Kalman-Filter schätzt die Zustände aufgrund von redundanten Daten. Die mögliche neue Position wird mit Systemeingangsdaten geschätzt und mit Messwerten verglichen. Die Differenz aus Schätzung und Messung wird schließlich gewichtet und dient als Korrektur des aktuellen Zustands.

Beim Partikelfilter wird die Wahscheinlichkeitsdicht durch die endliche Summe von Diracstößen mit Gewichten approximiert. Die Paare aus Gewicht und Zustand werden als Partikel betrachtet. Nach jedem Innovationsschritt werden schließlich die Gewichte aktualisiert.

\subsubsection{Sensordatenfusion}
\label{sec:Fusion}

Die Sensordatenfusion wird genutzt, um die Genauigkeit zu erhöhen bzw. mehr Informationen zu erhalten. Dies ist davon abhängig, welche Sensoren und wie sie eingesetzt werden. Im Allgemeinen werden die Ansätze in komplementär, konkurrierend und kooperativ unterschieden. Die Bedeutung für den Sensoreinbau bedeutet ist in Abbildung \ref{fig:Fusionsansaetze} dargestellt. Werden Sensoren komplementär genutzt, ergänzen sich deren einzelne Sichtfelder zu einem großen. Sind Sensoren konkurrierend verbaut, sind sie entweder redundant, d.h. es wird die gleiche Information generiert, oder konträr, d.h. es werden gegensätzliche Informationen erzeugt. Der letzte Ansatz ist der kooperative Einsatz von unterschiedlichen Sensoren, die zusammen einen höheren Informationsgehalt erzeugen \cite{Dietmayer2005}.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.6\textwidth,trim={3cm 6.5cm 9cm 4cm},clip]{pics/Fusionsansaetze.pdf}
	\caption{Ansätze der Datenaufnahme für die Datenfusion}
	\label{fig:Fusionsansaetze}
\end{figure}

Für die Sensordatenfusion gibt es zwei wesentliche Ansätze, siehe Abbildungen \ref{fig:ImpliziteFusion} und \ref{fig:ExpliziteFusion}. Entweder werden die Daten implizit oder explizit fusioniert. Bei der impliziten Fusion werden die Sensordaten zeitlich nacheinander eingebracht. Dadurch wird eine zeitlich konsistente Datenverarbeitung nötig. Außerdem muss eine zeitliche Filterung durchgeführt werden, wenn die Messdaten vorliegen. Es wird jedoch schon eine Assoziation auf dem sensorspezifischen Abstraktionslevel durchgeführt, die bei der Fusion abgeglichen wird. Vorteilhaft bei der impliziten Fusion ist, dass keine Synchronisierung der Sensoren durchgeführt werden muss.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{pics/ImpliziteFusion.PNG}
	\caption{Implizite Sensordatenfusion mit asynchronen Sensoren \cite{Dietmayer2005}}
	\label{fig:ImpliziteFusion}
\end{figure}

Bei der expliziten Fusion wird abgewartet, bis alle Messdaten vorliegen und dann erst fusioniert. Somit findet eine zeitliche Filterung in einem festen Zeitraster statt und die Assoziation findet auf einem gemeinsamen Abstraktionslevel statt. Hierbei müssen die Messdaten jedoch synchronisierbar sein.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{pics/ExpliziteFusion.PNG}
	\caption{Explizite Sensordatenfusion mit synchronen Sensoren \cite{Dietmayer2005}}
	\label{fig:ExpliziteFusion}
\end{figure}

Die Datenfusion mit synchronen Sensoren führt zu einer sicheren und zuverlässigen Assoziation. Da jedoch der Sensor mit der längsten Akquisitionszeit den zeitlichen Versatz zwischen Messung und Assoziation bestimmt, ist der Algorithmus streng deterministisch und es kommt zu einem hohen Verzug zwischen Realwelt und Modell. Die Arbeit mit synchronisierten Sensoren bietet jedoch, neben der sicheren Assoziation, eine einfache Erweiterbarkeit um weitere Sensoren.

Werden asynchrone Sensoren bei der Datenfusion verwendet, müssen die Daten sequentiell eingebracht werden. Somit ist der Algorithmus nicht deterministisch. Durch dieses Vorgehen kann es jedoch zu Quantisierungsfehlern kommen. Insbesondere, wenn die Daten des Sensors mit der kürzesten Latenz kurz nach denen vom Sensor mit der längsten Latenz eingebracht werden. Dann können diese Informationen nicht mehr in die Assoziation mitberücksichtigt werden. Je ähnlicher die Latenzzeiten der einzelnen Sensoren sind, desto geringer wird schließlich auch der Fehler der Schätzung. 

\subsection{Sensoreinsatz im Fahrzeug}
\label{sec:KFZSensor}

Im Fahrzeug werden die Sensoren zur Umfelderfassung so verbaut, dass ein möglichst großes Sichtfeld erzeugt wird. Eine Möglichkeit ist schematisch in Abbildung \ref{fig:Fahrzeugsensoren} dargestellt. Zu erkennen ist, dass das Long Range Radar und das Lidar in Fahrtrichtung genutzt wird. So kann eine vorausschauende Fahrweise realisiert werden. Short Range Radar, Kamera und Ultraschall dienen eher der Beobachtung des unmittelbaren Umfeldes.

\begin{figure}[hb]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 1cm 7.5cm 3cm},clip]{pics/Fahrzeugsensoren.pdf}
	\caption{Beispielhafte Darstellung der Umfelderfassungssensoren am Fahrzeug}
	\label{fig:Fahrzeugsensoren}
\end{figure}

Der Einsatz dieser Sensoren wird für die schrittweise Automatisierung des Fahrzeugs genutzt. Nach der Bundesanstalt für Straßenwesen wird in fünf Automatisierungsgrade unterschieden.\marginpar{Quelle einfügen} Diese sind in Tabelle \ref{tab:Automatisierungsgrade} aufgeführt und erläutert. Serienfahrzeuge bewegen sich mittlerweile im Automatisierungsgrad ''Teilautomatisiert''. 

\begin{table}[h]
	\centering
	\begin{tabularx}{\textwidth}{lX}
	\textbf{Nomenklatur} & \textbf{Fahraufgabe nach Automatisierungsgrad}\\ \toprule
	Driver Only & Fahrer führt dauerhaft Längs- und Querführung aus\vspace{8pt} \\ 
	Assistiert &  Fahrer führt dauerhaft entweder die Quer- oder die Längsführung aus. Die jeweils andere
	Fahraufgabe wird in gewissen Grenzen vom System ausgeführt
	\begin{itemize}\setlength{\itemsep}{0pt}
		\item Fahrer muss System überwachen
		\item Fahrer muss jederzeit für die vollständige Übernahme bereit sein
	\end{itemize}\\
	Teilautomatisiert & Das System übernimmt Quer- und Längsführung (für einen gewissen Zeitraum oder/und in spezifischen Situationen)
	\begin{itemize}\setlength{\itemsep}{0pt}
		\item Fahrer muss System überwachen
		\item Fahrer muss jederzeit für die vollständige Übernahme bereit sein
	\end{itemize}\\
	Hochautomatisiert & Das System übernimmt Quer- und Längsführung für einen gewissen Zeitraum in spezifischen Situationen
	\begin{itemize}\setlength{\itemsep}{0pt}
		\item Fahrer muss System nicht überwachen
		\item Fahrer wird rechtzeitig zur Übernahme aufgefordert
		\item Systemgrenzen werden alle erkannt
	\end{itemize}\\
	Vollautomatisiert & Das System übernimmt Quer- und Längsführung vollständig in einem definierten Anwendungsfall
	\begin{itemize}\setlength{\itemsep}{0pt}
		\item Fahrer muss System nicht überwachen
		\item Fahrer wird rechtzeitig vor verlassen des Anwendurngsfalles zur Übernahme aufgefordert
		\item Erfolgt keine Übernahme, wird das System in einen risikoarmen Zustand geführt
		\item Systemgrenzen werden alle erkannt
	\end{itemize}\\\bottomrule
\end{tabularx}
\caption{Fahrzeugautomatisierungsgrade nach der Bundesanstalt für Straßenwesen}
\label{tab:Automatisierungsgrade}
\end{table}

Die Automatisierung des Fahrzeugs wird mit Hilfe der sogenannten Fahrerassistenzsystemen durchgeführt \cite{Winner.2015}. Sie dienen vor allem der Sicherheit und erhöhen außerdem den Fahrkomfort. So kann beim Adaptive Cruise Control (ACC), das ein Long Range Radar oder ein Lidar nutzt, eine Wunschgeschwindigkeit eingestellt werden. Diese wird bei Annäherung an andere Fahrzeuge reduziert, um Auffahrunfälle zu vermeiden. Eine Erweiterung hierzu stellt der Stauassistent ACC Stop \& Go dar. Dieser nutzt insbesondere das Short Range Radar. Für Einparkassistenten werden Ultraschallsensoren oder Kameras genutzt. Hierbei werden entweder nur Tonsignale emittiert, eine grafische Darstellung im Boardcomputer gezeigt oder das Fahrzeug führt das Einparken vollautomatisch durch. Des Weiteren finden Spurhalte- und Spurwechselassistenten und Toter Winkel Assistenten Gebrauch, um Unfälle mit seitlich von Hinten herannahenden oder kreuzenden Verkehrsteilnehmern zu minimieren. Diese Assistenten nutzen Radar, Lidar oder Kamera und auch Kombinationen aus zwei dieser Sensoren.

In den vergangenen zehn Jahren wurde an verschiedenen Aspekten der Fahrzeugautomatisierung gearbeitet. Eine Auswahl an Arbeiten ist in Tabelle \ref{tab:LitFahrzeug} aufgeführt. Ein wichtiger Aspekt ist die Objekterkennung, bei der insbesondere die Erkennung von Menschen in den Vordergrund gerückt ist. Dies stellt mittels Radar aufgrund ihres geringen Reflexionsquerschnitts und der schlechten Winkelauflösung des Radars eine Herausforderung dar. Mögliche Ansätze wurden bei \cite{Ahtiainen.2010} und \cite{Bartsch.2012} erarbeitet. Auch die Objekterkennung mit Hilfe von Infrarotkameras findet immer mehr Einsatz \cite{Negied.2015}, \cite{Wang.2015}, da hiermit auch nachts die Kamera genutzt werden kann. Für die Verbesserung der Datenqualität wird auch an der Sensordatenfusion gearbeitet \cite{RudiLindl.2009}, \cite{Apatean.2013}.

Für das hochautomatisierte Fahrzeug gibt es verschiedene Ansätze. Hierbei spielt die Selbstlokalisierung eine wichtige Rolle, da das Fahrzeug jederzeit wissen muss, wo es sich befindet. Um dies zu realisieren werden neben den Umfelderfassungssensoren auch GPS, Beschleunigungs- und Geschwindigkeitssensoren und digitale Karten genutzt. So kann eine möglichst genaue Schätzung der Position durchgeführt werden. Einige Ansätze sind in \cite{Krzikalla2013}, \cite{Schindler.}, \cite{Lundgren.} und \cite{Vivacqua.2017} zu finden.


\begin{table}[h]
	\centering
		\begin{tabularx}{\textwidth}{p{2cm}p{2.5cm}Xc}
		\textbf{Quelle} & \textbf{Sensorsetup} & \textbf{Beschreibung} &  \textbf{Jahr}\\ \toprule
		\cite{RudiLindl.2009} & Radar\newline Lidar\newline Kamera & Sensorfusion für Fahrerassistenzsysteme mit hohen Ansprüchen  & 2009\\ \midrule
\cite{Perrone.2010} & Stereokamera & Modell zur Datenanalyse von Stereo Kameras  & 2010 \\ \midrule
\cite{Ahtiainen.2010} & 2 x Radar & Erkennen eines Menschen mit Radar  & 2010\\ \midrule
\cite{Bartsch.2012} & Radar & Erkennen eines Menschen mit Radar & 2012\\ \midrule
\cite{Krzikalla2013} & Lidar\newline GPS & Selbstlokalisierung mit Hilfe von GPS, Laserscanner und digitaler Karte &  2013\\ \midrule
\cite{Yalcin.2013} & Lidar & Positionierung des LIDAR, Fahrbahnbegrenzung erkennen, Objekterkennung  & 2013\\ \midrule
\cite{Apatean.2013} & IR\newline VIS Video & Fusioniert Infrarotkameradaten mit Daten des sichtbaren Spektrums einer Kamera  & 2013\\ \midrule
\cite{Schindler.} & Kamera\newline Lidar\newline GPS & Erzeugen einer Karte mit Hilfe von aktuellen Messdaten, Selbstlokalisierung des Fahrzeugs  & 2013\\ \midrule
\cite{Lundgren.} & GPS\newline Gyroscope\newline Kamera\newline Radar & KFZ ausgestattet mit GPS, Gyroscope, Geschwindigkeitsmesser, Kamera, Radar zur Selbstlokalisierung  & 2014\\ \midrule
\cite{Negied.2015} & IR & Literaturauflistung bzgl Erkennung von Menschen mit Infrarotsensor & 2015\\ \midrule
\cite{Wang.2015}   & FIR & stellt einen Filter zur Objekterkennung mit Infrarot vor &  2015\\ \midrule
\cite{Vivacqua.2017} & GPS\newline Gyroscope\newline Kamera & KFZ ausgestattet mit GPS, Gyroscope, Kamera und Laptop zur Selbstlokalisierung & 2017\\ \bottomrule
		\end{tabularx}
	\caption{Einsatz von Sensoren im Fahrzeug}
	\label{tab:LitFahrzeug}
\end{table}

\subsection{Sensoreinsatz in der Infrastruktur}
\label{sec:InfraSensor}

zur Unfallforschung\\
Verständnis von Fahrzeug-Fahrrad-Interaktion\\
unterstützend zum autonomen Fahren\\
C2X Kommunikation\\

Der Einsatz von Umfelderfassungssensoren in der Infrastruktur dient der Verkehrsbeobachtung. Insbesondere werden die aufgenommenen Daten zur Unfallforschung genutzt. So kann untersucht werden, welche Verhaltensmuster vermehrt zu Unfällen führen und weshalb es an gewissen Knotenpunkten häufig zu Unfällen kommt.\marginpar{Quelle?} Hierbei ist insbesondere die Interaktion zwischen KFZ bzw. LKW und Fahrrad interessant \cite{Dotzauer2017}. Außerdem wird der Einfluss der Witterung und der Tageszeit auf das Unfallgeschehen untersucht \cite{J.Ehrlichetal..2009}.

Der Tabelle \ref{tab:LitInfrastruktur} ist zu entnehmen, dass die Kamera bei der Verkehrsbeobachtung am häufigsten verwendet wird. Dies ist in der Tatsache begründet, dass sie eine ähnliche Darstellung wie das menschliche Auge erzeugt, siehe Abschnitt \ref{sec:Kamera}. Für die Bestimmung der Positionen wird sie meist mit Lidar oder Radar kombiniert.


\begin{table}
	\centering
		\begin{tabularx}{\textwidth}{p{2cm}p{2.5cm}Xc}
		\textbf{Quelle} & \textbf{Sensorsetup} & \textbf{Beschreibung} &  \textbf{Jahr}\\ \toprule
			\cite{J.Ehrlichetal..2009} & Kamera\newline Lidar \newline IR & CCTV, Laserscanner, IR zur Verkehrs- und Umweltbeobachtung (Bestimmung der Witterung, Tageszeit) &  2009\\ \midrule
\cite{Meissner.} & Lidar & Einsatz und Modellierung von Laserscannern an Kreuzungen &  2012\\ \midrule
\cite{Goldhammer2012} & Lidar\newline Kamera & Beschreibt den Versuchsaufbau an einer Kreuzung zur Beobachtung des Verkehrs &  2012\\ \midrule
\cite{Hospedales.2012} & Kamera & Vergleicht Algorithmen zur Verhaltenserkennung von Verkehrsobjekten in Kameradaten &  2012\\ \midrule
\cite{Goldhammer.18.09.2013} & Lidar\newline Kamera & Einsatz von Sensoren an einer Kreuzung (Anbauorte) &  2013\\ \midrule
\cite{EliasStrigel2013} & Lidar\newline Kamera & Einsatz von Kameras und Laserscanner an einer Kreuzung zur Beobachtung des Verkehrs  & 2013\\ \midrule
\cite{Strigel.} & Lidar\newline Kamera & Beschreibt den Versuchsaufbau an einer Kreuzung zur Beobachtung des Verkehrs & 2014\\ \midrule
\cite{Jodoin.} & Kamera & Stellt einen Algorithmus zum Objekttracking für die Bildverarbeitung vor, der an unterschiedlichen Kreuzungen getestet wurde &  2014\\ \midrule
\cite{DatondjiSokemiReneEmmanuel.2016} & Kamera & Listet und diskutiert Ansätze zur Verkehrsbeobachtung an Kreuzungen &  2016\\ \midrule
\cite{KnakeLanghorst.2016} & Kamera\newline IR\newline Radar & Stellt die Forschungsplatform AIM zur Untersuchung des Verkehrs vor &  2016\\ \midrule
\cite{Shirazi.2017} & GPS\newline Radar\newline Lidar\newline Kamera & Vergleicht verschiedene Ansätze der Verkehrsbeobachtung und Analyse & 2016\\ \midrule
\cite{Dotzauer2017} & Kamera\newline IR\newline Radar & Untersuchung von Konflikten zwischen Fahrradfahrern und motorisierten Fahrzeugen &  2017\\\midrule
\cite{Garcia2018} & Radar & Stellt \unit{76-81}{GHz} Radar zur Verkehrsüberwachung vor &  2018\\ \bottomrule
		\end{tabularx}
	\caption{Einsatz von Sensoren in der Infrastruktur}
	\label{tab:LitInfrastruktur}
\end{table}